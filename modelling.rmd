---
title: Modelling
output: bookdown::html_chapter
bibliography: references.bib
---

```{r data, include = FALSE}
source("common.r")
library(tidyr)

diamonds <- diamonds %>% tbl_df()
# Will be added to ggplot2 in due course
tx <- readRDS("tx-housing.rds")

tx <- tx %>%
  mutate(date = year + (month - 1) / 12) %>%
  filter(!(city %in% c("Texas Totals", "Palestine")))
```

# Modelling for visualisation {#cha:modelling}

Modelling is an essential tool for visualisation. Models are particularly powerful because they allow you to decompose patterns. If you see a strong pattern in the data, a model lets you pull it out. You can examine both the model, particularly useful when you have multiple individuals, and what remains.

There are two particularly strong connections between modelling and visualisation that I want to explore in this chapter:

* Using models as a tool to remove obvious patterns in your plots. This is 
  useful because strong patterns mask subtler effects. Often the strongest
  effects are already known and expected, and removing them allows you to 
  see surprises more easily.
  
* Other times you have a lot of data, too much to show on a handful of plots.  
  Models can be a powerful tool for summarising data so that you get a higher
  level few.

In this chapter, I'm going to focus on the use of linear models to acheive these goals. Linear models are a basic, but powerful tool of statistics, and I recommend that everyone serious about visualisation learns at least the basics of how to use them. To this end, I highly recommend the two book series by Julian J. Faraway:

* Linear Models with R <http://amzn.com/1439887330>
* Extending the Linear Model with R <http://amzn.com/158488424X>

These books cover the theory of linear models to some extent, but are pragmatic and show you how to actually use linear models (and their extensions) in R.

Of course, there are many other modelling tools other than linear models. I can't show them all here, but if you understand how linear models can help improve your visualisations, you should be able to port the basic idea to other families of models. This chapter just scratches the surface of what you can do. But hopefully it reinforces how visualisation can play combine with modelling to help you build a powerful data analysis toolbox. 

[Link to model-vis paper]

## Removing trend {#sub:trend}

So far our analysis of the diamonds data has been plagued by the powerful relationship between size and price. It makes it very difficult to see the impact of cut, colour and clarity because higher quality diamonds tend to be smaller, and hence cheaper. We can use a linear model to remove the effect of carat on price. Instead of looking at the raw price, we can look at the relative price: how valuable is this diamond relative to the average diamond of the same size.

To get started, we'll focus on diamonds of size two carats or less (96% of the dataset). This avoids some incidental problems that you can explore in the exercises, if you're interest. We'll also create two new variables: log price and log carat. These variables are useful because they produce a plot with a strong linear trend. A linear trend on a log-log scale suggests that there's an underlying power relationship between carat and price: larger diamonds are not just linearly more expensive than smaller diamonds.

```{r}
diamonds2 <- diamonds %>% 
  filter(carat <= 2) %>%
  mutate(
    lcarat = log(carat),
    lprice = log(price)
  )

ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE, size = 2)
```

(If you're wondering what the horizontal gap with no points is, go back to Section XYZ.)

In the graphic above we used `geom_smooth()` to overlay the line of best fit to the data. We can replicate this outside of ggplot2 by fitting a linear model with `lm()`. This allows us to get the slope and intercept of the line:

```{r}
mod <- lm(lprice ~ lcarat, data = diamonds2)
coef(summary(mod))
```

If you've used linear models before you might jump to start interpreting those coefficients: $\log(price) = 8.5 + 1.7 * \log(carat)$. Or with a little algebra, $price =  4900 * carat ^ 1.7$. Interpreting those coefficients certainly is useful, but even if you don't understand them, the model can still be useful. We can use it to substract the trend away by looking at the residuals: the price minus the predicted, or expected, price. Geometrically, the residuals are the vertical distance between each point and the line of best fit. The residuals give us the price relative to the "average" diamond of that size.

```{r, dev = "png"}
diamonds2 <- diamonds2 %>% mutate(rel_price = resid(mod))
ggplot(diamonds2, aes(carat, rel_price)) + geom_point()
```

A relative price of zero means that the diamond was at the average price; positive means that it's means that it's more expensive than expected, and negative means that it's cheaper than expected. 

Interpreting the values precisely is a little tricky here because we've log-transformed price. The residuals give the absolute difference ($x - expected$), but here we have $log(price) - log(expected price)$, or equivalently $log(price / expected price)$. If we exponentiate the residuals we get $price / expected price$. You could back-transform it, to be more interpretable, but that would lose the nice properties of the log ratio, namely that it's symmetric (i.e. both relatively cheaper and relatively more expensive diamonds have the same range). We can make a little table to help interpret the values:

```{r, echo = FALSE}
xgrid <- seq(-1.5, 1.5, length = 11)
knitr::kable(data.frame(logx = xgrid, x = exp(xgrid)))
```

For example, here a relative price of 0.6 means that it's 1.8x of the expected price (i.e 80% more expensive than expected); a relative price of -0.3 means that it's 0.74x of the expected price (i.e. 26% cheaper than expected).

Let's compare the use both price and relative price to see how color and cut affect the value of a diamond. We'll compute the average price and average relative price for each combination of colour and cut:

```{r}
color_cut <- diamonds2 %>% 
  group_by(color, cut) %>%
  summarise(
    price = mean(price), 
    rel_price = mean(rel_price)
  )
```

If we look at price, it's hard to see how the quality of the diamond affects the price. The lowest quality diamonds (fair cut with colour J) have the highest average value! This is because those diamonds also tend to be larger: size and quality are confounded.

```{r}
ggplot(color_cut, aes(color, price)) + 
  geom_line(aes(group = cut), color = "grey80") +
  geom_point(aes(colour = cut), size = 3)
```

If however, we plot the relative price, you see the pattern that you expect: as the quality of the diamonds decreases, the relative price decreases. The worst quality diamond is 0.61x ($exp( -0.49)) the price of an "average" diamond.

```{r}
ggplot(color_cut, aes(color, rel_price)) + 
  geom_line(aes(group = cut), color = "grey80") +
  geom_point(aes(colour = cut), size = 3)
```

This technique can be employed in a wide range of situations. Wherever you can explicitly model a strong pattern that you see in a plot, it's worthwhile to use a model to remove that strong pattern so that you can see what interesting trends remain. 

### Exercises

1.  What happens if you repeat the above analysis with all diamonds? (Not just 
    all diamonds with two or fewer carats). What does the strange geometry of
    `log(carat)` vs relative price represent? (Or alternatively, what does the
    diagonal line without any points represent?)

1.  I made an unsupported assertion that lower-quality diamonds tend to 
    be larger. Support my claim with a plot.

1.  Can you create a plot that simultaneously shows the effect of colour,
    cut, and clarity on relative price? If there's too much information to
    show on one plot, think about how you might create a sequence of plots
    to convey the same message.

1.  How do depth and table relate to the relative price? How do the patterns
    compare to the untransformed price?

## effects

The **effects** package [@effects] is particularly useful for extracting these values from linear models.  The following example fits a two-way model with interaction, and shows how to extract and visualise marginal and conditional effects.  Figure \ref{fig:model-cat} focusses on the categorical variable colour, and Figure \ref{fig:model-cont} focusses on the continuous variable carat. \index{Package!effects}

```{r effects, prompt=TRUE, fig.align='left'} 
d <- subset(diamonds, carat < 2.5)
d$lcarat <- log10(d$carat)
d$lprice <- log10(d$price)

# Remove overall linear trend
detrend <- lm(lprice ~ lcarat, data = d)
d$lprice2 <- resid(detrend)

mod <- lm(lprice2 ~ lcarat * color, data = d)

library("effects")
effectdf <- function(...) {
  suppressWarnings(as.data.frame(effect(...)))
}
color <- effectdf("color", mod)
both1 <- effectdf("lcarat:color", mod)

carat <- effectdf("lcarat", mod, default.levels = 50)
both2 <- effectdf("lcarat:color", mod, default.levels = 3)

qplot(lcarat, lprice, data=d, colour = color)
qplot(lcarat, lprice2, data=d, colour = color)

```

```{r ldata, echo=FALSE, dev='png', fig.show='hold', fig.cap="Data transformed to remove most obvious effects. (Left) Both x and y axes are log10 transformed to remove non-linearity. (Right) The major linear trend is removed."} 
```

```{r model-cat, echo=FALSE, fig.show='hold', fig.cap="Displaying uncertainty in model estimates for colour. (Left) Marginal effect of colour. (Right) conditional effects of colour for different levels of carat. Error bars show 95\\% pointwise confidence intervals."} 
fplot <- ggplot(mapping = aes(y = fit, ymin = lower, ymax = upper)) +
  ylim(range(both2$lower, both2$upper))
fplot %+% color + aes(x = color) + geom_point() + geom_errorbar()
fplot %+% both2 + 
  aes(x = color, colour = lcarat, group = interaction(color, lcarat)) +
  geom_errorbar() + geom_line(aes(group=lcarat)) +
  scale_colour_gradient()
```

```{r model-cont, echo=FALSE, fig.show='hold', fig.cap="Displaying uncertainty in model estimates for carat. (Left) marginal effect of carat. (Right) conditional effects of carat for different levels of colour. Bands show 95\\% point-wise confidence intervals."} 
fplot %+% carat + aes(x = lcarat) + geom_smooth(stat="identity")

ends <- subset(both1, lcarat == max(lcarat))
fplot %+% both1 + aes(x = lcarat, colour = color) +
 geom_smooth(stat="identity") + 
 scale_colour_hue() + theme(legend.position = "none") +
 geom_text(aes(label = color, x = lcarat + 0.02), ends)
```

Note, when captioning such figures, you need to carefully describe the nature of the confidence intervals, and whether or not it is meaningful to look at the overlap.  That is, are the standard errors for the means or for the differences between means?  The packages **multcomp** and **multcompView** are useful for calculating and displaying these errors while correctly adjusting for multiple comparisons. \index{Package!multcomp} \index{Package!multcompView}


## Texas housing data

To explore one connection between modelling and visualisation in more depth, we're going to use a new dataset: TX housing data. This data was collected from the pages provided by the Real Estate Center at Texas A&M University, <http://recenter.tamu.edu/Data/hs/>. The data contains information about 46 Texas cities, recording the number of house sales (`sales`), the total volume of sales (`volume`), the `average` and `median` sale prices, the number of houses listed for sale (`listings`) and the average number of months a house has been listed (`inventory`). Data is recorded monthly from Jan 1990 to Apr 2015, 304 entries for each city.

```{r}
tx
```

We're going to explore how sales have varied over time for each city as it shows some interesting trends and poses some interesting challenges. Let's start with an overview: a time series of sales for each city:

```{r, fig.width = 5, fig.height = 3}
ggplot(tx, aes(date, sales)) + 
  geom_line(aes(group = city))
```

Two factors make it hard to see the long-term trend in this plot:

1.  The range of sales varies over multiple orders of magnitude. The biggest 
    city, Houston, averages over ~4000 sales per month; the smallest city, San 
    Marcos, only averages ~20 sales per month.
    
1.  There is a strong seasonal trend: sales are much higher in the summer than
    in the winter.

We can fix the first problem by plotting the log of sales:
    
```{r, fig.width = 5, fig.height = 3}
ggplot(tx, aes(date, log(sales))) + 
  geom_line(aes(group = city), alpha = 1/2)
```

We can fix the second problem using the same technique we used for removing the trend in the diamonds data: we'll fit a linear model and look at the residuals. This time we'll use a categorical predictor to remove the month effect. First we check that the technique works by applying it to a single city. It's always a good idea to start simple so that if something goes wrong you can more easily pinpoint the problem.

```{r, fig.show="hold", fig.width = 3, fig.height = 2}
abilene <- tx %>% filter(city == "Abilene")
ggplot(abilene, aes(date, log(sales))) + 
  geom_line()

mod <- lm(log(sales) ~ factor(month), data = abilene)
ggplot(abilene, aes(date, resid(mod))) + 
  geom_line()
```

We can apply this transformation to every city with `group_by()` and `mutate()`. Note the use of `na.action = na.exclude` argument to `lm()`. Counterintuitively this ensures that missing values in the input are matched with missing values in the output predictions and residuals. Without this argument, missing values are just dropped, and the residuals don't line up with the inputs.

```{r}
tx <- tx %>% 
  group_by(city) %>% 
  mutate(resid = resid(lm(log(sales) ~ factor(month), na.action = na.exclude)))
```

With this data in hand, we can re-plot the data. Now that we have log-transformed the data and removed the strong seasonal affects we can see there is a strong common pattern: a consistent increase from 1990-2007, a drop until 2010 (with quite some noise), and then a gradual rebound. To make that more clear, I included a summary line that shows the mean residual for each date across all cities. 

```{r, fig.width = 5, fig.height = 3}
ggplot(tx, aes(date, resid)) +
  geom_line(aes(group = city), alpha = 1/5) + 
  geom_line(stat = "summary", fun.y = "mean", colour = "red")
```

(Note that removing the seasonal effect also removed the intercept - we see the trend for each city relative to the average number of sales.)

### Exercises

1.  The final plot shows a lot of short-term noise in the overall trend. How
    could you smooth this further to focus on long-term changes?
    
1.  If you look closely (e.g. `+ xlim(2005, 2015)`) at the long-term trend 
    you'll notice a weird pattern in 2009-2011. It looks like there was a big dip in 2010. Is this dip "real"?
    (i.e. can you spot it in the original data)

1.  What other variables in the TX housing data show strong 
    seasonal effects? Does this technique help to remove them?
  
1.  Not all the cities in this data set have complete time series.
    Use your dplyr skills to figure out how much data each city
    is missing. Display the results with a visualisation.

1.  Replicate the computation that `stat_summary()` did with dplyr so
    you can plot the data directly.

## Visualising models {#sub:modelvis}

In the previous examples we used the linear model just as a tool for removing trend. We used the model and immediately threw it away. We didn't care about the model itself, just what it could do for us. But the models themselves contain useful information and if we keep them around, there are many new problems that we can solve:

* We might be interested in cities where the model didn't not fit well:
  a poorly fitting model suggests that there isn't much of a seasonal pattern,
  which contradicts our implicit hypothesis that all cities share a similar 
  pattern.

* The coefficients themselves might be interesting. In this case, looking
  at the coefficients will show us how the seasonal pattern varies between
  cities.
  
* We may want to dive into the details of the model itself, and see exactly 
  what it says about each observation. In this data, it might help us find 
  suspicious data points that might reflect data entries errors.
  
To take advantage of this data, we need to store the models. We can do this using another dplyr verb: `do()`. It allows us to store the result of arbitrary computation in a column. Here we'll use it to store that linear model:

```{r}
models <- tx %>% 
  group_by(city) %>%
  do(mod = lm(
    log(sales) ~ factor(month), 
    data = ., 
    na.action = na.exclude
  ))
models
```

There are two important things to note in this code:

* `do()` creates a new column called `mod.` This is special type of column: 
  rather than containing an atomic vector (a logical, integer, numeric, 
  or character) like usual, it's a list. Lists are R's most flexible data 
  structure and can hold anything, including linear models.

* `.` is a special pronoun used by `do()`. It refers to the "current" data 
  frame. In this example, `do()` fits the model 46 times (once for each 
  city), each time replacing `.` with the data for one city.
  
If you're an experienced modeller, you might wonder why I didn't fit one model to all cities simultaneously. That's a great next step, but it's often useful to start off simple. Once we have a model that works for each city individually, you can figure out how to generalise it to fit all cities simultaneously.
  
To visualise these models, we'll turn them into tidy data frames. We'll do that with the __broom__ package by David Robinson. 

```{r}
library(broom)
```

Broom provides three key verbs, each corresponding to one of the challenges outlined above:

* `glance()` makes __model__-level summaries with one row of data for each 
  model. It contains summary statistics like the $R^2$ and degrees of freedom.

* `tidy()` makes __coefficient__-level summaries with one row of data for each 
  coefficient in each model. It contains information about individual 
  coefficients like their estimate and standard error.

* `augment()` makes __observation__-level summaries with one row of data for 
  each observation in each model. It includes variables like the residual and 
  influence metrics useful for diagnosing outliers.

We'll learn more about each of these functions in the following three sections.

## Model-level summaries

We'll begin by looking at how well the model fit to each city with `glance()`:

```{r}
model_sum <- models %>% glance(mod)
model_sum
```

This creates a variable with one row for each city, and variables that either summarises complexity (e.g. `df`) or fit (e.g. `r.squared`, `p.value`, `AIC`). Since all the models we fit have the same complexity (12 terms: one for each month), we'll focus on the model fit summaries. $R^2$ is a reasonable place to start because it's well known. We can use a dot plot to see the variation across cities:

```{r, fig.width = 4, fig.height = 6}
ggplot(model_sum, aes(r.squared, reorder(city, r.squared))) + 
  geom_point()
```

It's hard to picture exactly what those values of $R^2$ mean for this data, so I pick out the three cities with the highest and lowest $R^2$:

```{r, fig.width = 6, fig.height = 6}
top3 <- c("Midland", "Irving", "Denton County")
bottom3 <- c("Brownsville", "Harlingen", "McAllen")
extreme <- tx %>% ungroup() %>%
  filter(city %in% c(top3, bottom3), !is.na(sales)) %>%
  mutate(city = factor(city, c(top3, bottom3)))

ggplot(extreme, aes(month, log(sales))) + 
  geom_line(aes(group = year)) + 
  facet_wrap(~city)
```

The cities with low $R^2$ have weaker seasonal patterns and more variations between years. The data for Harlingen seems particularly noisy.

### Exercises

1.  Do your conclusions change if you use a different measurement of model fit?
    Why/why not?
    
1.  One possible hypothesis that explains why McAllen, Harlingen and Brownsville
    have lower $R^2$ is that they're smaller towns so there are fewer sales and 
    more noise. Confirm or refute this hypothesis.
    
1.  McAllen, Harlingen and Brownsville seem to have much more year-to-year
    variation than Midland, Irving and Denton County. How does the model
    change if you also include a linear trend for year? (i.e. 
    `log(sales) ~ factor(month) + year`). 

1.  Create a faceted plot that shows the seasonal patterns for all cities.  
    Order the facets by the $R^2$ for the city.

## Coefficient-level summaries

The model fit summaries suggest that there are some important differences in seasonality between the different cities. Lets dive into those differences by using `tidy()` to extract detail about each individual coefficient:

```{r}
coefs <- models %>% tidy(mod)
coefs
```

We're more interested in the month effect, so we'll do a little extra tidying to only look at the month coefficients, and then to extract the month value into a numeric variable:

```{r}
months <- coefs %>%
  filter(grepl("factor", term)) %>%
  extract(term, "month", "(\\d+)", convert = TRUE)
months
```

This is a common pattern. You need to use your data tidying skills at many points in an analysis. Once you have the correct tidy dataset, creating the plot is usually easy. Here we'll put month on the x axis, estimate on the y-axis, and draw one line for each city. I've back-transformed (exponentiated) to make the coefficients more interpretable: these are now ratios of sales compared to January.

```{r}
ggplot(months, aes(month, exp(estimate))) +
  geom_line(aes(group = city))
```

The pattern seems similar across the cities. The main difference is the strength of the seasonal effect. Let's pull that out and plot it:

```{r, fig.width = 4, fig.height = 6}
coef_sum <- months %>%
  group_by(city) %>%
  summarise(max = max(estimate))
ggplot(coef_sum, aes(max, reorder(city, max))) + 
  geom_point()
```

The cities with the strongest seasonal effect are College Station and San Marcos (both college towns) and Galveston and South Padre Island (beach cities). It makes sense that these cities would have very strong seasonal effects.

### Exercises

1.  Pull out the three cities with highest and lowest seasonal effect. Plot
    their coefficients.
    
1.  How does strength of seasonal effect relation to the $R^2$ for the model?
    Answer with a plot.
    
    ```{r, echo = FALSE, eval = FALSE}
    coef_sum %>% 
      left_join(model_sum %>% select(city, r.squared)) %>% 
      ggplot(aes(max, r.squared)) + 
        geom_point() + 
        geom_smooth(method = "lm", se = F)
    ```

1.  Group the diamonds data by cut, clarity and colour. Fit a linear model
    `log(price) ~ log(carat)`. What does the intercept tell you? What does
    the slope tell you? How do the slope and intercept vary across the
    groups? Answer with a plot.
    
## Observation data

Observation level data, which include residual diagnostics, is most useful in the traditional model fitting scenario. It's less useful in conjunction with visualisation, but we can still find interesting patterns. It's particularly useful if we want to track down individual values that seem particularly odd. Extracting observation level data is the job of the `augment()` function. This adds one row for each observation. It includes the variables used in the original model, and a number of common influence statistics (see `?augment.lm` for more details):

```{r, warning=FALSE}
obs_sum <- models %>% augment(mod)
obs_sum
```

For example, it might be interesting to look at the distribution of standardised residuals. (These are residuals standardised to have a variance of one in each model, making them more comparable). We're looking for unusual values that we might have nt o dive into in more detail.

```{r}
ggplot(obs_sum, aes(.std.resid)) + 
  geom_histogram(binwidth = 0.1)
ggplot(obs_sum, aes(abs(.std.resid))) + 
  geom_histogram(binwidth = 0.1)
```

A threshold of 2 seems like a reasonable threshold to explore in more detail. 

```{r}
obs_sum %>% 
  filter(abs(.std.resid) > 2) %>%
  group_by(city) %>%
  summarise(n = n(), avg = mean(abs(.std.resid))) %>%
  arrange(desc(n))
```

This is a quick high level summary that suggests cities that we might want to look at into more detail.

### Exercises

1.  A common diagnotic plot is fitted values (`.fitted`) vs. residuals 
    (`.resid`). Do you see any patterns? What if you include the city 
    or month on the same plot?
    
1.  Create a time series of log(sales) for each city. Highlight points that have
    a standardised residual of greater than 2.

## Learning more

This chapter has only scratched the surface of the intersection between visualisation and modelling. In my opinion, mastering the combination of visualisations and models is key being an effective data scientist. Unfortunately most books (like this one!) only focus on either visualisation or modelling, but not both. There's a lot of interesting work to be done.

 We're only going to use broom with linear models, but it can handle many others including `glm`, `lme4`, `survival` and `multcomp`. Check the documentation to see if it handles your data.
 
I'd also recommend you learn more about the broom package. We've only used it for linear models, but it supports many other types of model as well, and support will continue to grow over time. Start at the README on <https://github.com/dgrtwo/broom>.