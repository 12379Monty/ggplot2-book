---
title: Modelling
output: bookdown::html_chapter
bibliography: references.bib
---

```{r data, echo = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
options(digits = 2, width = 60, dplyr.print_min = 5, dplyr.print_max = 5)
knitr::opts_chunk$set(comment = "#>", compact = TRUE)

diamonds <- diamonds %>% tbl_df()
# Will be added to ggplot2 in due course
tx <- readRDS("tx-housing.rds")

tx <- tx %>%
  mutate(date = year + (month - 1) / 12) %>%
  filter(!(city %in% c("Texas Totals", "Palestine")))
```

# Modelling for visualisation {#cha:modelling}

Modelling is an essential tool for visualisation. Models are particularly power because they allow you to decompose patterns. If you see a strong pattern in the data, a model lets you pull it out. You can examine both the model, particularly useful when you have multiple individuals, and what remains.

It has two main uses that I'll discuss in this chapter:

* Removing strong trends so that you can see the subtler patterns that remain.
  Or removing the effect of confounding variables so you can see the smaller 
  effects that remain.

* Summarising large amounts of data by fitting many models and visualise 
  the summary statistics of those models.

Another powerful use of modelling for visualisation is to transform the data in way that is more easily seen:

* Very large datasets can be summarised (using the ideas above). 

* Very high dimensionality datasets may need to be modified just to 
  get something you can see (e.g. with an ordination or a seriation.)
  
* Models (particularly Bayesian) can allow you to combine prior knowledge
  with scant data.
  
* ...

Unforuntately I don't have enough space to explain modelling in depth. But if you're familiar with linear models from other sources (e.g. X, Y & Z), this should allow you to deploy them in creative ways to improve your visualisations. If you don't know anything about linear models, I strongly encourage you to learn something. They'll help many of your visualisations show what's really important.

If you're new to linear models, I highly recommend the two book series by Julian J. Faraway:

* Linear Models with R <http://amzn.com/1439887330>
* Extending the Linear Model with R <http://amzn.com/158488424X>

This chapter just scratches the surface of what you can do. But hopefully it reinforces how visualisation can play combine with modelling to help you build a powerful data analysis toolbox.

## Removing trend {#sub:trend}

Throughout this book, our analysis of the diamonds data has been plagued by the powerful relationship between size and price. It makes it very difficult to see the impact of the other c's (cut, colour and clarity) because higher quality diamonds tend to be smaller, and hence cheaper. We can use a linear model to remove the effect of carat on price, and instead of looking at the raw price, look at the adjusted price: how valuable this diamond is relative to the average diamond of that size.

We'll focus on diamonds two carats or less. This is 96% of the dataset. We'll also need to log transform both price and carat - this converts from a power relationship to a linear relationship:

```{r}
diamonds2 <- diamonds %>% 
  filter(carat <= 2) %>%
  mutate(
    lcarat = log(carat),
    lprice = log(price)
  )

ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_point()
```

(If you're wondering what the horizontal gap with no points is, go back to Section XYZ.)

We'll now fit a linear model to this data

```{r}
mod <- lm(lprice ~ lcarat, data = diamonds2)
coef(summary(mod))
```

If you have a classical statistics training, you might start by trying to interpret those model coefficients. With a little algebra we can verify that if $ln(price) = a + b * ln(carat)$ then $price = exp(a) * carat ^ b$. So this model tells us that $price = 4734 * carat ^ 1.69$.

But we're not going to interpret the model - we're just going to use it to subtract the trend out of the main plot. We're going to compute the residuals, given us the relative price. A plot of carat vs relative price shows that we've succeeded in removing the strong trend.

```{r, dev = "png"}
diamonds2 <- diamonds2 %>% 
  mutate(
    rel_price = resid(mod)
  )
qplot(carat, rel_price, data = diamonds2)
```

A relative price of zero represents same as average price for that carat. Negative price means cheaper than expected. Positive means more expensive than the average.

Normally the residuals give the absolute difference ($x - y$), but since we've log transformed the data it's going to be a relative difference ($log(x / y)$). You could back-transform it, to be more interpretable, but that would lose the nice properties of the log ratio, namely that it's symmetric (i.e. both relatively cheaper and relatively more expensive diamonds have the same range). We can make a little table to help interpret the values:

```{r, echo = FALSE}
xgrid <- seq(-1.5, 1.5, length = 11)
knitr::kable(data.frame(logx = xgrid, x = exp(xgrid)))
```

Now that we have the relative price, it's much easier to see the impact of the cut, colour and clarity of the diamonds. The pattern of the price isn't terribly strong, but it suggests that as the quality decreases, the average price _increases_. This is driven by lower quality diamonds tending to be larger. If instead we look at the relative price, you see the pattern that you expect: as the quality of the diamonds decreases, the relative price decreases.

```{r}
color_cut <- diamonds2 %>% 
  group_by(color, cut) %>%
  summarise(
    price = mean(price), 
    rel_price = mean(rel_price)
  )

ggplot(color_cut, aes(color, price)) + 
  geom_line(aes(group = cut), color = "grey80") +
  geom_point(aes(colour = cut), size = 3)

ggplot(color_cut, aes(color, rel_price)) + 
  geom_line(aes(group = cut), color = "grey80") +
  geom_point(aes(colour = cut), size = 3)
```

This technique can be employed in a wide range of situations. Wherever you can explicitly model a strong pattern that you see in a plot, it's worthwhile to use a model to remove that strong pattern so that you can see what interesting trends remain. In the diamonds data, it really is essential to focus on relative price.

### Exercises

1.  There's an area in the top-right of the plot of carat vs. relative price 
    that doesn't have any points in it. What does that area represeent? 

1.  I made an unqualitied assertion that lower-quality diamonds tend to 
    be larger. Support my claim with a plot.

1.  Can you create a plot that simultaneously shows the effect of colour,
    cut and clarity on relative price? If there's too much information to
    show on one plot, think about how you might create a sequence of plots
    to convey the same message.

1.  How do depth and table relate to the relative price? How do the patterns
    compare to the untransformed price?

## Texas housing data

To explore one connection between modelling and visualisation in more depth, we're going to use a new dataset: TX housing data. This data was collected from the pages provided by the Real Estate Center at Texas A&M University, <http://recenter.tamu.edu/Data/hs/>.

```{r}
tx
```

The data contains information about 46 Texas cities, recording the number of house sales (`sales`), the total volume of sales (`volume`), the `average` and `median` sale prices, the number of houses listed for sale (`listings`) and the average number of months a house has been listed (`inventory`). Data is recorded monthly from Jan 1990 to Apr 2015, 304 entries for each city.

We're going to explore how sales have varied over time for each city. This shows some interesting trends and poses some interesting challenges. Let's start with an overview: plotting sales across time for each city.

```{r, fig.height = 2, fig.width = 4}
ggplot(tx, aes(date, sales)) + 
  geom_line(aes(group = city))
```

Two factors make this plot difficult to understand: 

1.  The range of sales varies over multiple orders of magnitude. The 
    biggest city, Houston, has over averages over 4000 sales per month; 
    the smallest city, San Marcos, only averages 20 sales per month.
    
1.  There is a strong seasonal trend within the year: sales are 
    highest in the summer and lowest in the winter.

We can ameliorate the first problem by switching to log scales:
    
```{r, fig.height = 2, fig.width = 4}
ggplot(tx, aes(date, log(sales))) + 
  geom_line(aes(group = city), alpha = 1/2)
```

The fix the second problem, we can remove the month effect using the same basic techique we used to remove the strong trend in the diamonds data. We'll use a linear model again, but this time we'll use a categorical predictor to remove the month effect.

First we check that technique it works by applying it to a single city. It's always a good idea to start simple so that if something goes wrong you can more easily pin-point the problem.

```{r, fig.keep="hold"}
abilene <- tx %>% filter(city == "Abilene")
ggplot(abilene, aes(date, log(sales))) + 
  geom_line()

mod <- lm(log(sales) ~ factor(month), data = abilene)
ggplot(abilene, aes(date, resid(mod))) + 
  geom_line()
```

We can apply this transformation to every city with `group_by()` and `mutate()`. Note the use of `na.action = na.exclude` argument to `lm()`. Counterintuitively this actually ensures that missing values in the input are matched with missing values in the output (preidictions and residuals). Without this argument, missing values are just dropped, and the residuals don't line up with the inputs.

```{r}
tx <- tx %>% 
  group_by(city) %>% 
  mutate(resid = resid(lm(log(sales) ~ factor(month), na.action = na.exclude)))
```

With this data in hand, we can re-plot the data. Now that we have log-transformed the data and removed the strong seasonal affects we can see there is a strong common pattern: a consistent increase from 1990-2007, a drop until 2010 (with quite some noise), and then a gradual rebound. To make that more clear, I included a summary line that shows the mean residual for each date across all cities. 

```{r}
ggplot(tx, aes(date, resid)) +
  geom_line(aes(group = city), alpha = 1/5) + 
  geom_line(stat = "summary", fun.y = "mean", colour = "red")
```

### Exercises

1.  What other variables in the TX housing data show strong 
    seasonal effects? Does this technique help to remove them?
  
1.  Not all the cities in this data set have complete time series.
    Use your dplyr skills to figure out how much data each city
    is missing. Display the results with a visualisation.

1.  Replicate the computation that `stat_summary()` did with dplyr so
    you can plot the data directly.

## Visualising models {#sub:modelvis}

In the example above we used a linear model as a tool for removing trend. We didn't care about the model itself, just what it could do for us. We used the model and immediately threw it away. But the model itself contains useful information! If we kept the models around, there is a lot of interesting questions we can answer:

* We might be interested in cities where the model didn't not fit well:
  a poorly fitting model suggests that there isn't much of a seasonal pattern,
  which contradicts our initial hypothesis that all cities share a similar 
  pattern.

* The coefficients themselves might be interesting. In this case, looking
  at the coefficients will show us how the seasonal pattern varies between
  cities.
  
* We may want to dive into the details of the model itself, and
  see exactly what it says about each observation. In this data,
  it might help us find suspicious data points that might reflect
  data entries errors.
  
To take advantage of this data, we need to store the models. We can do this using another dplyr verb: `do()`. It allows us to store the result of arbitrary computation in a column. Here we'll use it to store that linear model:

```{r}
models <- tx %>% 
  group_by(city) %>%
  do(mod = lm(log(sales) ~ factor(month), data = ., na.action = na.exclude))
models
```

There are two important things to note in this code:

* `.` is a special pronoun used by `do()`. It refers to a data frame containing
  a single group. In this case, `do()` fits the model 46 times (once for each 
  city), each time replacing `.` with a data frame containing just one city.
  
* The `do()` command create a new column called mod. This is special type of 
  column: rather than containing an atomic vector (a logical, integer, numeric, 
  or character) like most data frame columns, it's a list. Lists are R's 
  flexible data structure which can hold anything, including linear models.

To visualise these models, we need to turn them into data frames so you can apply everything you already know about visualising data with ggplot2. We're going to do that with the __broom__ package by David Robinson. It provides three key verbs, each corresponding to a challenge outlined above:

* `glance()` makes one row of data for each model. It contains summary 
  statistics like the $R^2$ and degrees of freedom.

* `tidy()` makes one row of data for each coefficient in each model. It contains
  information about individual coefficients like their estimate and standard 
  error.

* `augment()` makes one row of data for each observation in each model. It 
  includes variables like the residual and influence metrics useful for 
  diagnosing outliers.

We'll learn more about each of these functions in the following three sections. We're only going to use broom with linear models, but it can handle many others including `glm`, `lme4`, `survival` and `multcomp`. Check the documentation to see if it handles your data.

## Model level summaries

We'll begin by looking at how well the model fit to each city. We use `glance()` on our model data frame, giving it the column that contains the model:

```{r}
library(broom)
model_sum <- models %>% glance(mod)
model_sum
```

Each model summary either summarises the model complexity (e.g. `df`) or model fit (e.g. `r.squared`, `p.value`, `AIC`). Since all the models we fit have the same complexity (12 terms: one for each month), we'll focus on the model fit summaries, and $R^2$ is a reasonable place to start because it's well known. We can use a dot plot to see the variation across cities:

```{r}
ggplot(model_sum, aes(r.squared, reorder(city, r.squared))) + 
  geom_point()
```

To help picture what this means, I next pick out the three cities with the highest and lowest $R^2$:

```{r}
top3 <- c("Midland", "Irving", "Denton County")
bottom3 <- c("McAllen", "Harlingen", "Brownsville")
extreme <- filter(tx, city %in% c(top3, bottom3))
ggplot(extreme, aes(month, log(sales))) + 
  geom_line(aes(group = year)) + 
  facet_wrap(~city)
```

The cities with low $R^2$ indeed have weak seasonal patterns. The data for Harlingen in particular seem to be very noisy.

### Exercises

1.  Do your conclusions change if you use a different model fit measure?
    Why/why not?
    
1.  One possible hypothesis to explain why McAllen, Harlingen and Brownsville
    have less seasonal pattern is that they're smaller towns so there are
    fewer sales and more noise. Confirm or refute this hypothesis.
    
1.  McAllen, Harlingen and Brownsville seem to have much more year-to-year
    variation than Midland, Irving and Denton County. How does the model
    fit change if you also include a linear trend for year in the model?
    (i.e. `log(sales) ~ factor(month) + year`).

1.  Create a faceted plot that shows the seasonal patterns for all cities.  
    Order the facets by the $R^2$ for the city.

## Coefficients

The model fit summaries suggest that there are some important differences in seasonality between the different cities. Lets dive into those differences by using `tidy()` to extract the coefficient level data:

```{r}
coefs <- models %>% tidy(mod)
coefs
```

We're more interested in the month effect, so we'll do a little extra tidying to only look at the month coefficients, and then to extract the month value into a numeric variable:

```{r}
months <- coefs %>%
  filter(grepl("factor", term)) %>%
  extract(term, "month", "(\\d+)", convert = TRUE)
months
```

This is a common pattern. You need to use your data tidying skills at many points in an analysis. Once you have the correct tidy dataset, creating the plot is usually easier. Here we'll put month on the x axis, and estimate on the y-axis, drawing one line for each city. I've back-transformed (exponentiated) to make the coefficients more interpretable: these are now ratios of sales compared to January.

```{r}
ggplot(months, aes(month, exp(estimate))) +
  geom_line(aes(group = city))
```

The pattern seems similar across the cities. The main difference is the strength of the seasonal effect. Let's pull that out and plot it:

```{r}
coef_sum <- months %>%
  group_by(city) %>%
  summarise(max = max(estimate))
ggplot(coef_sum, aes(max, reorder(city, max))) + 
  geom_point()
```

The cities with the strongest seasonal effect are College Station and San Marcos (both college towns) and Galveston and South Padre Island (beach cities). It makes sense that these cities would have very strong seasonal effects.

### Exercises

1.  Pull out the three cities with highest and lowest seasonal effect. Plot
    their coefficients.
    
1.  How does strength of seasonal effect relation to the $R^2$ for the model?
    Answer with a plot.
    
    ```{r, echo = FALSE, eval = FALSE}
    coef_sum %>% 
      left_join(model_sum %>% select(city, r.squared)) %>% 
      ggplot(aes(max, r.squared)) + 
        geom_point() + 
        geom_smooth(method = "lm", se = F)
    ```

1.  Group the diamonds data by cut, clarity and colour. Fit a linear model
    `log(price) ~ log(carat)`. What does the intercept tell you? What does
    the slope tell you? How do the slope and intercept vary across the
    groups? Answer with a plot.
    
## Observation data

Observation level data, which include residual diagnostics, is most useful in the traditional model fitting scenario. It's less useful in conjunction with visualisation, but we can still find interesting patterns. It's particularly useful if we want to track down individual values that seem particularly odd.

```{r, warning=FALSE}
obs_sum <- models %>% augment(mod)

ggplot(obs_sum, aes(.fitted, .resid)) + 
  geom_point()

obs_sum %>% 
  filter(abs(.resid) > 1.5) %>%
  group_by(city) %>%
  summarise(n = n())
```

Often useful as prequel to fitting a single model that encompasses all cities.

## Learning more

* The [broom README](https://github.com/dgrtwo/broom), gives a more detailed
  overview of broom and points you to the latest resources on where to learn 
  more.
