\input{_header.tex}

% SET_DEFAULTS
%   GG-WIDTH: 4  GG-HEIGHT: 4
%   TEX-WIDTH: 0.5\linewidth
%   CACHE: TRUE
%   INLINE: FALSE
% 
% options(digits = 2)

% END

\chapter{Manipulating data}
\label{cha:data}

The next two sections focus on data.  In Section~\ref{sec:plyr}, you will learn how to use the \pkg{plyr} package to reproduce the statistical transformations performed by the layers, and then in Section~\ref{sec:melting} you will learn a little about ``molten'' (or long) data, which is useful for time series and parallel coordinates plots, among others.  Section~\ref{sec:methods} shows you how to write methods that let you plot objects other than data frames, and demonstrates how \ggplot can be used to recreate a more flexible version of the built in linear-model diagnostics.  

\section{An introduction to plyr}
\label{sec:plyr}

With faceting, ggplot2 makes it very easy to identical plots for different subsets of your data. This section introduces \f{ddply} from the \pkg{plyr} package which makes it easy to do the same thing for numerical summaries. \pkg{plyr} provides a comprehensive suite of tools for breaking up complicated data structures into pieces, processing each piece and then joining the results back together. The \pkg{plyr} package as a whole provides tools for breaking and combining lists, arrays and data frames. Here we will focus on the \f{ddply} function which breaks up a data frame into subsets based on row values, applies a function to each subset, and the joins the results back into a data frame. The basic syntax is \code{ddply(.data, .variables, .fun, ...)}, where

\begin{itemize}
  \item \code{.data} is the dataset to break up (the data that you are plotting).
  
  \item \code{.variables} is a description of the variables to break up the dataset by.  This is written like \code{.(var1, var2)}, and to match the plot should contain all grouping and faceting variables used in the plot.
  
  \item \code{.fun} is the summary function you want to use.  The function can return a vector or data frame.  The result does not need to contain the grouping variables: these will be added on automatically if they're needed.  The result can be a much reduced aggregated dataset (maybe even one number), or the original data modified or expanded in some way.

\end{itemize}

More information and examples are available in the documentation, \code{?ddply}, and on the package website, \url{http://had.co.nz/plyr}.  The following examples show a few useful summary functions that solve common data manipulation problems.

\begin{itemize}
  \item Using \f{subset} allows you to select the top (or bottom) n (or x\%) of observations in each group, or observations above (or below) some group specific threshold:
  
  % LISTING
  % 
  % # Select the smallest diamond in each colour
  % ddply(diamonds, .(color), subset, carat == min(carat))
  % 
  % # Select the two smallest diamonds
  % ddply(diamonds, .(color), subset, order(carat) < 2)
  % 
  % # Select the 1% largest diamonds in each group
  % ddply(diamonds, .(color), subset, carat > quantile(carat, 0.99))
  % 
  % # Select all diamonds bigger that the group average
  % ddply(diamonds, .(color), subset, price > mean(price))
  \input{_include/39a855ab4e46bcdb417c3ec11540bfd1.tex}  
  % END
  
  \item Using \f{transform} allows you to perform group-wise transformations with very little work.  This is particularly useful if you want to add new variables that can are calculated on a per group level, such as a per-group standardisation.  Section~\ref{sub:time-series} shows another use of this technique for standardising time series to a common scale.
  
  % LISTING
  % 
  % # Within each colour, scale price to have mean 0 and variance 1
  % ddply(diamonds, .(color), transform, price = scale(price))
  % 
  % # Subtract off group mean
  % ddply(diamonds, .(color), transform, price = price - mean(price))
  \input{_include/bfc86a4c688874ab2531722ea6467d6d.tex}  
  % END

  \item If you want to apply a function to every column in the data frame, you might find the \f{colwise} function handy. This function converts a function that operates on vectors to a function that operates column-wise on data frames. This is rather different to most functions: instead of returning a vector of numbers, \f{colwise} returns a new function. The following example creates a function to count the number of missing values in a vector and then shows how we can use \f{colwise} to apply it to every column in a data frame.

  % INTERWEAVE
  % 
  % nmissing <- function(x) sum(is.na(x))
  % nmissing(msleep$name)
  % nmissing(msleep$braint)
  % 
  % nmissing_df <- colwise(nmissing)
  % nmissing_df(msleep)
  % # This is shorthand for the previous two steps
  % colwise(nmissing)(msleep)
  
   The specialised version \f{numcolwise} does the same thing, but works only with numeric columns.  For example, \code{numcolwise(median)} will calculate a median for every numeric column, or \code{numcolwise(quantile)} will calculate quantiles for every numeric column.  Similarly, \f{catcolwise} only works with categorical columns.
  
  % INTERWEAVE
  % 
  % numcolwise(median)(msleep, na.rm = T)
  % numcolwise(quantile)(msleep, na.rm = T)
  % numcolwise(quantile)(msleep, probs = c(0.25, 0.75), na.rm = T)
  \input{_include/6f4f39926aa611a1f933f8b3bbee2bad.tex}  
  % END
  
  Combined with \code{ddply}, this makes it easy to produce per-group summaries:
  
  % INTERWEAVE
  % 
  % ddply(msleep, .(vore), numcolwise(median), na.rm = T)
  % ddply(msleep, .(vore), numcolwise(mean), na.rm = T)
  \input{_include/5d9000102f1c8cae5cdc97e6a52b4a5a.tex}  
  % END
  
  \item If none of these shortcuts is appropriate, make your own summary function which takes a data frame as input and returns an appropriately summarised data frame as output.  The following functions calculates the rank correlation of price and carat and compares it to the regular correlation of the logged values.
  
  % INTERWEAVE
  % 
  % my_summary <- function(df) {
  %   with(df, data.frame(
  %     pc_cor = cor(price, carat, method = "spearman"),
  %     lpc_cor = cor(log(price), log(carat))
  %   ))
  % }
  % ddply(diamonds, .(cut), my_summary)
  % ddply(diamonds, .(color), my_summary)
  \input{_include/99ca1a8404499dc4b4b47ef6620f8a29.tex}  
  % END
  
  Note how our summary function did not need to output the group variables.  This makes it much easier to aggregate over different groups.
\end{itemize}

The common pattern of all these problems is that they are easy to solve if we have the right subset.  Often the solution for a single case might be a single line of code.  The difficulty comes when we want to apply the function to multiple subsets and then correctly join back up the results.  This may take a lot of code, especially if you want to preserve group labels.  \f{ddply} takes care of all this for you.

The following case study shows how you can use \pkg{plyr} to reproduce the statistical summaries produced by \ggplot.  This is useful if you want to save them to disk or apply them to other datasets.  It's also useful to be able to check that \ggplot is doing exactly what you think!

\subsection{Fitting multiple models}
\label{sub:multiple_models}

In this section, we'll work through the process of generating the smoothed data produced by \code{stat_smooth}.  This process will be the same for any other statistic, and should allow you to produce more complex summaries that \ggplot can't produce by itself.  Figure~\ref{fig:smooth} shows the group-wise smoothes produced by the following code.

% FIGLISTING
%   LABEL: smooth
%   CAPTION: A plot showing the smoothed trends for price vs carat for each
%   colour of diamonds.  With the full range of carats (left), the standard
%   errors balloon after around two carats because there are relatively few
%   diamonds of that size.  Restricting attention to diamonds of less than
%   two carats (right) focuses on the region where we have plenty of data.
% 
% qplot(carat, price, data = diamonds, geom = "smooth", colour = color)
% dense <- subset(diamonds, carat < 2)
% qplot(carat, price, data = dense, geom = "smooth", colour = color)
\input{_include/20dd23c7ef16dd1a194b2df3be1e6cf9.tex}
% END

How can we recreate this by hand?  First we read the \f{stat_smooth} documentation to determine what the model is: for large data it's \code{gam(y ~ s(x, bs = "cs"))}.  To get the same output as \f{stat_smooth}, we need to fit the model, then predict it on an evenly spaced grid of points.  Figure~\ref{smooth-by-hand} shows the results, which are identical to what we got with \ggplot doing all the work.

% FIGLISTING
%   LABEL: smooth-by-hand
%   CAPTION: Figure~\ref{fig:smooth} with all statistical calculations 
%   performed by hand.  The predicted values (left), and with standard errors
%   (right).
% 
% library(mgcv)
% smooth <- function(df) {
%   mod <- gam(price ~ s(carat, bs = "cs"), data = df)
%   grid <- data.frame(carat = seq(0.2, 2, length = 50))
%   pred <- predict(mod, grid, se = T)
%   
%   grid$price <- pred$fit
%   grid$se <- pred$se.fit
%   grid
% }
% smoothes <- ddply(dense, .(color), smooth)
% qplot(carat, price, data = smoothes, colour = color, geom = "line")
% qplot(carat, price, data = smoothes, colour = color, geom = "smooth",
%   ymax = price + 2 * se, ymin = price - 2 * se)
\input{_include/905ab1ad6286c0662fe475b59df96c8f.tex}
% END

This gives us much more flexibility to fit models where the group is explicitly included as a covariate.  For example, the following model models price as a non-linear function of carat, plus a constant term for each colour.  It's not a very good model as it predicts negative prices for small, poor-quality diamonds, but it's a starting point for a better model.

% INTERWEAVE
% 
% mod <- gam(price ~ s(carat, bs = "cs") + color, data = dense)
% grid <- with(diamonds, expand.grid(
%   carat = seq(0.2, 2, length = 50),
%   color = levels(color)
% ))
% grid$pred <- predict(mod, grid)
% qplot(carat, pred, data = grid, colour = color, geom = "line")
\input{_include/5100e9bbea15a2a40b78a0201b58bc57.tex}
% END

See also Sections~\ref{sub:different_aesthetics} and \ref{sec:uncertainty} for other ways of combining models and data.

\section{Converting data from wide to long}
\label{sec:melting}

In \ggplot graphics, groups are defined by rows, not by columns.  This makes it easy to draw a line for each group defined by the value of a variable (or set of variables) but difficult to draw a separate line for each variables.  In this section you will learn how to transform your data to a form in which you can draw line for each variable.  This transformation converts from ``wide'' data to ``long'' data, where each variable now occupies it's own set of rows.

To perform this transformation we will use the \f{melt} function from the \pkg{reshape} package \citep{wickham:2007b}.  Reshape also provides the \f{cast} function to flexibly reshape and aggregate data, which you may want to read about yourself.   Table~\ref{tbl:melt} gives an example.  The \f{melt} function has three arguments:

\begin{itemize}
  \item \code{data}: the data frame you want to convert to long form.

  \item \code{id.vars}: Identifier (id) variables identify the unit that measurements take place on.  Id variables are usually discrete, and are typically fixed by design.  In {\sc anova} notation ($Y_{ijk}$), id variables are the indices on the variables ($i, j, k$); in database notation, id variables are a composite primary key.

  \item \code{measure.vars}: Measured variables represent what is measured on that unit ($Y$).  These will be the variables that you want to display simultaneously on the plot.
\end{itemize}

If you're familiar with Wilkinson's grammar of graphics, you might wonder why there is no equivalent to the algebra.  There is no equivalent to the algebra within \ggplot itself because there are many other facilities for transforming data in R, and it is inline with my philosophy of keeping data transformation and visualisation as separate as possible.

% library(xtable)
% xtable(format(head(economics), digits = 2)[, 1:3])
% em <- melt(head(economics), id = "date", m = c("pce", "pop"))
% xtable(format(em, digits = 2))

\begin{table}[ht]
  \centering
  \begin{minipage}[t]{0.4\linewidth}
  \begin{tabular}{rrr}
    \toprule
    date & pce & pop \\
    \midrule
    1967-06-30 & 508 & 198,712 \\
    1967-07-31 & 511 & 198,911 \\
    1967-08-31 & 517 & 199,113 \\
    1967-09-30 & 513 & 199,311 \\
    1967-10-31 & 518 & 199,498 \\
    1967-11-30 & 526 & 199,657 \\
    \bottomrule
  \end{tabular}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[t]{0.4\linewidth}
  \begin{tabular}{rrr}
    \toprule
    date & variable & value \\
    \midrule
    1967-06-30 & pce &     508 \\
    1967-07-31 & pce &     511 \\
    1967-08-31 & pce &     517 \\
    1967-09-30 & pce &     513 \\
    1967-10-31 & pce &     518 \\
    1967-11-30 & pce &     526 \\
    1967-06-30 & pop & 198,712 \\
    1967-07-31 & pop & 198,911 \\
    1967-08-31 & pop & 199,113 \\
    1967-09-30 & pop & 199,311 \\
    1967-10-31 & pop & 199,498 \\
    1967-11-30 & pop & 199,657 \\
    \midrule
  \end{tabular}
  \end{minipage}

  \caption{Economics data in wide, left, and long, right, formats.  The data stored in each table is equivalent, just the arrangement is different.  It it easy to use the wider format with \ggplot to produce a line for each variable.}
  \label{tbl:melt}
\end{table}

The following sections explore two important uses of molten data in more detail: plotting multiple time series and creating parallel coordinate plots. You will also learn how to use \pkg{plyr} to rescale the variables, and learn about the features of \ggplot that are most useful in conjunction with this sort of data.

\subsection{Multiple time series}
\label{sub:time-series}

Take the \code{economics} data set.  It contains information about monthly economic data like the number of people unemployed (\code{unemploy}) and the median length of time a person is unemployed (\code{uempmed}).  We might expect these two variables to be related.  Each of these variables is stored in a column, which makes it easy to compare them with a scatterplot, and draw individual time series, as shown in Figure~\ref{fig:series-wide}.  But what if we want to see them collectively?

% FIGURE
%   COL: 3  TEX-WIDTH: 0.33\linewidth
%   LABEL: series-wide
%   CAPTION: When the economics data set is stored in wide format, it is easy
%   to create separate time series plots for each variable (left and centre),
%   and easy to create scatterplots comparing them (right).
% 
% qplot(date, uempmed, data = economics, geom = "line")
% qplot(date, unemploy, data = economics, geom = "line")
% qplot(unemploy, uempmed, data = economics) + geom_smooth()
\input{_include/85fdaf61d98be680d39981b65992ee24.tex}
% END

One way is to build up the plot with a different layer for each variable.   However, that quickly becomes tedious when you have more than a few variables.  An alternative is to melt the data into a long format. Then the two time series have their value stored in the \var{value} variable and we can distinguish between them with the \var{variable} variable.  The code below shows these two alternatives.  The plots they produce are very similar, and are shown in Figure~\ref{fig:series-methods}.

% FIGLISTING
%   LABEL: series-methods GG-HEIGHT: 3 GG-WIDTH: 6
%   CAPTION: The two methods of displaying both series on a single plot 
%   produce identical plots, but using long data is much easier when you have
%   many variables.  The series have radically different scales, so we only
%   see the pattern in the \code{unemploy} variable. You might not even notice
%   \code{unempmed} unless you're paying close attention: it's the line at the
%   bottom of the plot.
% 
% ggplot(economics, aes(date)) + 
%   geom_line(aes(y = unemploy, colour = "unemploy")) + 
%   geom_line(aes(y = uempmed, colour = "umempmed")) + 
%   scale_colour_hue("variable")
%
% emp <- melt(economics, id = "date", measure = c("unemploy", "uempmed"))
% qplot(date, value, data = emp, geom = "line", colour = variable)
\input{_include/c4ced5c6879fae3935934e1b17002549.tex}
% END

There is a problem with these plots: the two variables have radically different scales, and so the series for \code{unempmed} appears as a flat line.  There is no way to produce a plot with two axis in \ggplot (as I believe that this type of plot is fundamentally misleading).  There are two alternatives: rescale the variables to share common ranges, or facet with free scales.  These alternatives are created with the code below and are shown in Figure~\ref{fig:series-scaling}

% FIGLISTING
%   LABEL: series-scaling 
%   GG-HEIGHT: 3 GG-WIDTH: 6
%   CAPTION: When the series have very different scales we have two 
%   alternatives: left, rescale the variables to a common scale, or right,
%   display the variables on separate facets and using free scales.
% 
% range01 <- function(x) {
%   rng <- range(x, na.rm = TRUE)
%   (x - rng[1]) / diff(rng)
% }
% emp2 <- ddply(emp, .(variable), transform, value = range01(value))
% qplot(date, value, data = emp2, geom = "line", 
%   colour = variable, linetype = variable)
% qplot(date, value, data = emp, geom = "line") + 
%   facet_grid(variable ~ ., scales = "free_y")
\input{_include/7201700a9c50b45e5f5b37396debbee6.tex}
% END

\subsection{Parallel coordinates plot} 
\label{sub:molten_data}

In a similar manner, we can use molten data to create a parallel coordinates plot \citep{inselberg:1985,wegman:1990}, which has the ``variable'' variable on the x axis and value on the y axis.  We need a new variable to record the row that each observation came from, which is used as a grouping variable for the lines (so we get one line per observation). The easiest value to use for this is the data frame \code{rownames}, and we give it an unusual name {\code{.row}) so we don't squash any of the existing variables). Once we have the data in this form, creating a parallel coordinates plot is easy.

In this case study, we will explore the ratings of around 840 movies with over 10,000 votes.  This data set has a moderate number of variables (10) and many cases, and will allow us to experiment with a common technique for dealing with large data in parallel coordinates plots: transparency and clustering.

This data is already on a common scale, so we don't need to rescale it, but in general, we would need to use the technique from the previous section to ensure the variables are comparable.  This is particularly important if we are going to use other multidimensional techniques to analyse the data.

% FIGLISTING
%   LABEL: pcp
%   FILETYPE: PNG
% 
% popular <- subset(movies, votes > 1e4)
% ratings <- popular[, 7:16]
% ratings$.row <- rownames(ratings)
% molten <- melt(ratings, id = ".row")
% 
% pcp <- ggplot(molten, aes(variable, value, group = .row))
% pcp + geom_line()
% pcp + geom_line(colour = alpha("black", 1 / 20))
% pcp + geom_line(colour = alpha("black", 1 / 20), position = "jitter")
\input{_include/e53b8dc010762eac2ef590c86e8f3a48.tex}
% END

% FIGLISTING
% 
% cl <- kmeans(ratings[1:10], 5)
% ratings$cluster <- reorder(factor(cl$cluster), popular$rating)
% levels(ratings$cluster) <- seq_along(levels(ratings$cluster))
% molten <- melt(ratings, id = c(".row", "cluster"))
% 
% pcp_cl <- pcp %+% molten + scale_colour_hue(alpha = 1/10)
% pcp_cl + geom_line(aes(colour = cluster), position ="jitter")
% pcp_cl + geom_line(aes(colour = cluster)) + facet_wrap(~ cluster)
% 
% ratings$avg <- cut_number(popular$rating, 5)
% molten <- melt(ratings, id = c(".row", "cluster", "avg"))
% pcp_avg <- pcp %+% molten
% pcp_avg + geom_line(aes(colour = cluster), position="jitter") + facet_grid(cluster ~ avg) + scale_colour_hue(alpha = 0.1)
% pcp_avg + stat_summary(aes(colour = cluster, group = cluster, fill=cluster), fun.y = mean, fun.ymin = min, fun.ymax = max, geom="smooth")
% pcp_avg + geom_line(colour=alpha("black", 1/10)) + stat_summary(aes(colour = cluster, group = cluster), fun.y = mean, geom="line", size = 2) + facet_wrap(~ cluster)

% Dealing with missing values in parallel coordinates plots is a little tricky.  One way is just to set missing values to a number outside of the usual range:
% 
% % INTERWEAVE
% % 
% % scaled$value[is.na(molten$value)] <- -0.2
% % pcp %+% scaled
% \input{_include/809b64d7ca992a30abaa5341f623693a.tex}
% % END
% 
% Because we have total control of the data transformation, it is easy to experiment with variations on the parallel coordinate plot.  We could use different types of rescaling, like standardising to mean 0 and standard deviation 1, or calculating ranks (this is sometimes called a bump chart).  Or we could use different geoms, like boxplots, to display the data.  Instead of converting categorical variables to numeric, we can treat them as id variable, and then assign them to aesthetics, or facet by them.
% 
% % INTERWEAVE
% % 
% % msleep2$brainwt <- log(msleep2$brainwt)
% % msleep2$bodywt <- log(msleep2$bodywt)
% % molten <- melt(msleep2, id = ".row")
% % scaled <- ddply(molten, .(variable), transform, value = range01(value))
% % missing <- transform(scaled, value = ifelse(is.na(value), -0.2, value))
% % pcp %+% missing
% % 
% % pcp %+% missing %+% 
% %   geom_boxplot(aes(group = variable), fill = alpha("white", 0.75), 
% %    colour = alpha("#3366CC", 0.75), data = scaled)
% \input{_include/80cfdebffe1f1a715f8ad9a3710d5e8d.tex}
% % END

\section{\f{ggplot} methods}
\label{sec:methods}

\f{ggplot} is a generic function, with different methods for different types of data. The most common, and what we have used until now, are data frames. However, as with base and lattice graphics, it is possible to extend \f{ggplot} to work with other types of data. However, the way this works is fundamentally different: \ggplot will not give you a complete plot, but instead will give you the tools to make the plot.

This process is mediated by the \f{fortify} method, which takes an object, and optional data frame, and creates a version of the object in a form suitable for plotting with \ggplot, i.e.\ as a data frame. The name fortify comes from thinking about combining a model with its data: the model fortifies the data, and the data fortifies the model, and the result can be used to simultaneously visualise the model and the data. An example will make this concrete, as you will see when we describe the fortify method for linear models.

This section describes how the \f{fortify} method works, and how you can create new methods that are aligned with the \ggplot philosophy.  The most important philosophical consideration is that data transformation and display should be kept as separate as possible.  This maximises reusability, as you are no longer trapped into the single display that the author envisaged.  

These different types of input also work with \f{qplot}: remember that \f{qplot} is just a thin wrapper around \f{ggplot}

\subsection{Linear models}

Currently, \ggplot provides only one fortify method, for linear models. Here we'll show how this method works, and how you can use it to create tailored plots for better understanding your data and models. Figure~\ref{fig:plot-lm} shows the output of \f{plot.lm} for a simple model. The graphics produces a set of pre-chosen model summary plots. These are useful for particular problems, but are completely inflexible: there is no way to modify them apart from opening up the source code for \f{plot.lm} and modifying it. The code is difficult to understand because data transformation and display are inextricably entangled.

% mod <- lm(cty ~ displ, data = mpg)
% pdf("data-plot-lm.pdf", width = 8, height = 8)
% par(mfrow = c(2, 2))
% plot.lm(mod)
% dev.off()

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{data-plot-lm}
  \caption{The output from \f{plot.lm} for a simple model.  }
  \label{fig:plot-lm}
\end{figure}

The \ggplot approach completely separates data transformation and display.  The \f{fortify} method does the transformation, and then we use \ggplot as usual to create the display that we want.  At the time of printing, \f{fortify} adds the variables in Table~\ref{tbl:fortify-vars} to the original dataset.  These are basically all the variables that \f{plot.lm} collects in order to create its summary plots.  The variables have a leading \code{.} (full stop) in their names, so there is little risk that they will clobber variables already in the dataset.  

\begin{table}
  \centering
  \begin{tabular}{lp{2.5in}}
    \toprule
    Variable & Description \\
    \midrule
    \code{.cooksd}   & Cook's distances \\
    \code{.fitted}   & Fitted values \\
    \code{.hat}      & Diagonal of the hat matrix \\
    \code{.resid}    & Residuals \\
    \code{.sigma}    & Estimate of residual standard deviation when corresponding observation is dropped from model \\
    \code{.stdresid} & Standardised residuals \\
    \bottomrule
  \end{tabular}
  \caption{The diagnostic variables that \f{fortify.lm} assembles and adds to the model data.}
  \label{tbl:fortify-vars}
\end{table}

% If we just supply \f{fortify} with the model, it will add the diagnostic columns to the model data frame (which just contains the variables used in the model), or we can also supply the full original dataset.  

To demonstrate these techniques, we're going to fit the very simple model shown below and visualised in Figure~\ref{fig:fortify-mod}.  This model clearly doesn't fit the model very well, so we should be able to use model diagnostics to figure out how to improve it.  A sample of the output from fortifying this model is shown in Table~\ref{tbl:fortify-out}.  Because we supply the original data frame it contains the two variables used in the model and the six diagnostic variables.  It's easy to see exactly what data our plot will be working with and we could easily add more variables if we wanted.

% FIGLISTING
%   LABEL: fortify-mod
%   CAPTION: A simple linear model that doesn't fit the data very well.
% 
% qplot(displ, cty, data = mpg) + geom_smooth(method = "lm")
% mpgmod <- lm(cty ~ displ, data = mpg)
\input{_include/d2e8624db875296a00f0fbf231a54ca3.tex}
% END

% library(xtable)
% xtable(head(fortify(mpgmod)))

\begin{table}[ht]
  \centering
  \begin{tabular}{rrrrrrrrr}
    \toprule
    cty & displ & .hat & .sigma & .cooksd & .fitted & .resid & .stdresid \\
    \midrule
    18 & 1.80 & 0.01 & 2.56 & 0.01 & 21.26 & -3.26 & -1.28 \\
    21 & 1.80 & 0.01 & 2.57 & 0.00 & 21.26 & -0.26 & -0.10 \\
    20 & 2.00 & 0.01 & 2.57 & 0.00 & 20.73 & -0.73 & -0.29 \\
    21 & 2.00 & 0.01 & 2.57 & 0.00 & 20.73 & 0.27 & 0.11 \\
    16 & 2.80 & 0.01 & 2.57 & 0.00 & 18.63 & -2.63 & -1.03 \\
    18 & 2.80 & 0.01 & 2.57 & 0.00 & 18.63 & -0.63 & -0.24 \\
    \bottomrule
  \end{tabular}
  \caption{The output of \code{fortify(mpgmod)} contains the two variables used in the model (\var{cty} and \var{displ}), and the six diagnostic variables described above.}
  \label{tbl:fortify-out}
\end{table}

% You may notice some similarity between this approach and the transformations performed by stats.  The major difference is that \f{fortify} is global, while statistical transformations are local to the facet and group.

With a fortified data set in hand we can easily recreate the plots produced by \f{plot.lm}, and even better, we can adapt them to our needs.  The example below shows how we can recreate and then extend the first plot produced by \f{plot.lm}.  Once we have the basic plot we can easily enhance it: use standardised residuals instead of raw residuals, or make size proportional to Cook's distance.  The results are shown in Figure~\ref{fig:fortify-fr}

% FIGLISTING
%   COL: 3 TEX-WIDTH: 0.33\linewidth
%   LABEL: fortify-fr
%   CAPTION: Basic fitted values-residual plot, left.  With standardised
%   residuals, centre.  With size proportional to Cook's distance, right.  It
%   is easy to modify the basic plots when we have access to all of the data.
% 
% mod <- lm(cty ~ displ, data = mpg)
% basic <- ggplot(mod, aes(.fitted, .resid)) +
%   geom_hline(yintercept = 0, colour = "grey50", size = 0.5) + 
%   geom_point() + 
%   geom_smooth(size = 0.5, se = F)
% basic
% basic + aes(y = .stdresid)
% basic + aes(size = .cooksd) + scale_area("Cook's distance")
\input{_include/48bb0d8780e76f569009d0c9752cd36b.tex}
% END

Additionally, we can fortify the whole dataset and add to the plot variables that are in the original data but not in the model.  This helps us to understand what variables are useful to improve the model.  Figure~\ref{fig:fortify-full} colours the residuals by the number of cylinders, and suggests that this variable would be good to add to the model: within each cylinder group, the pattern is linear.

% FIGLISTING
%   LABEL: fortify-full
%   CAPTION: Adding variables from the original data can be enlightening.  
%   Here when we add the number of cylinders we see that instead of a 
%   curvi-linear relationship between displacement and city mpg, it is 
%   essentially linear, conditional on the number of cylinders. 
% 
% full <- basic %+% fortify(mod, mpg)
% full + aes(colour = factor(cyl))
% full + aes(displ, colour = factor(cyl))
\input{_include/d3106162075a607c4a8417a4aa114e70.tex}
% END

\subsection{Writing your own}

If you are writing your own fortify method, you will need to think about what variables are most useful.  The method for linear models adds them on to the original data frame, but this might not be the best approach in other circumstances, and you may want to return a list of data frames giving information at different levels of aggregation.

You can also use \f{fortify} with non-model functions.  The following example shows how we could write a \f{fortify} method to make it easier to add images from disk to your plots.  The \pkg{EBImage} from bioconductor is used to get the image into R, and then the fortify method converts it into a form (a data frame) that \ggplot can render.

% LISTING
% 
% fortify.Image <- function(model, data, ...) {
%   colours <- channel(model, "x11")[,,]
%   colours <- colours[, rev(seq_len(ncol(colours)))]
%   melt(colours, c("x", "y"))
% }
% 
% library(EBImage)
% img <- readImage("http://euclid.psych.yorku.ca/SCS/Gallery/images/Private/Langren/google-toledo-rome3.jpg", TrueColor)
% qplot(x, y, data = img, fill = value, geom="tile") + 
%   scale_fill_identity()
\input{_include/60650a06a68b4c3cd73d3c706c255134.tex}
% END


The approach described in section cleanly separates the display of the data from its production, and dramatically improves reuse.  However, it does not provide any conveniently pre-packaged functions.  If you want to create a diagnostic plot for a linear model you have to assemble all the pieces yourself.  Once you have the basic structure in place, so that people can always dig back down and alter the individual pieces, you can write a function that joins all the components together in a useful way.

\input{_footer.tex}
