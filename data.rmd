---
title: data
output: bookdown::html_chapter
bibliography: references.bib
---

```{r data, echo = FALSE, message = FALSE}
library("ggplot2")
library("plyr")
library("dplyr")
library("tidyr")
library("xtable")
options(digits = 2, width = 60)
```

# Data analysis {#cha:data}

So far, every example in this book has started with a nice, tidy, dataset. This is great for learning ggplot2 because you don't want to struggle with getting data in the right shape at the same time as you learn how to use ggplot2. However, if you want to use ggplot2 with your own data, you'll need to learn some new techniques. In this chapter, you'll learn the principles of tidy data, which help you organise your data in a way that makes it easy for ggplot2 to work with. You'll also learn a little about about dplyr, which helps you manipulate tidy data, as is common during the course of a visualisation.

Indeed, in my experience, visualisation is often the easiest part of this process: once you have tidy data, aggregated to the most useful level and modelled to discover the most important trends, the right visualisation is often quite easy.

The goal of this chapter is to show you how to integrate visualisation with ggplot2 with the other parts of the data analysis process:

* ggplot2 makes it easy to map variables to visual properties, but it depends 
  on a particular way of storing your data. This form (or shape) is called tidy
  data. In XXX you'll learn the basics of tidy data and how you can make your
  messy data tidy with the __tidyr__ package.
  
* Most visualisations also require some data transformation whether it's 
  creating a new variable from existing variables, or performing simple 
  aggregations so you can see the forest for the tree. Section XXX will teach 
  how to use the __dplyr__ package to make this as easy as possible.
  
* If you're using R, you're almost certainly using it for it's fantastic
  modelling capabilities. While there's an R package for almost every type
  of model that you can think of, the results of these models can be hard to
  visualise. The __broom__ package, by David Robinson, makes your life easier 
  by converting model outputs into standard tidy datasets so you can easily
  integrate with ggplot2.

Data cleaning, manipulation and transformation is a big topic and this chapter only scratches the surface of topics closely related to `ggplot`.  I recommend the following references which go into considerably more depth on this topic:

* "[Tidy data](http://www.jstatsoft.org/v59/i10/)", an article in the _Journal
  of Statistical Software_. It describes the ideas of tidy data in more depth
  and shows other types of messy data. Unfortunately the paper was written
  before tidyr existed, so to see how to use tidyr instead of reshape2, consult
  the 
  [tidyr vignette](http://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html).

* The dplyr vignettes, which you can see with 
  `browseVignettes(package = "dplyr")`, go into considerably more depth into
  dplyr. There you'll learn how to also use dplyr with data frames, and how
  to work with multiple tables of data.

* The [broom README](https://github.com/dgrtwo/broom), gives a more detailed
  overview of broom and points you to the latest resources on where to learn 
  more.
  
* RStudio provides a number of cheatsheets at 
  <http://www.rstudio.com/resources/cheatsheets/>. These are a great way to 
  remember the most commonly used functions.

## Tidy data {#sec:tidy-data}

The principle behind tidy data is simple: storing your data in a consistent way makes it easier to work with it. Tidy data is a mapping between statistical structure of a data frame (variables and observations) with the physical structure (columns and rows).

1. Variables should go in columns.
1. Observations should go in rows.

Tidy data is particularly important for ggplot2 because the job of ggplot2 is to map variables to visual properties. Assuming a consistent way of accessing variables makes ggplot2 much easier to use, but sometimes you'll hit a dataset that you have no idea how to plot. That's normally because it's not tidy: the variables aren't provided as columns, so you have no way to tell ggplot2 what to plot.

For example, take this data frame that contains monthly employment data for the United States. This data contains three variables: month, year and unemployment rate:

```{r, echo = FALSE, message = FALSE}
library("lubridate")
ec2 <- 
  ggplot2::economics %>% 
  tbl_df() %>%
  transmute(year = year(date), month = month(date), rate = uempmed) %>%
  filter(year > 2000) %>%
  spread(year, rate)
ec2
```

(If it looks familiar it's because it's dervied from the `economics` dataset included in ggplot2.)

Imagine you want to plot a time series showing how unemployment has changed over the last 10 years? Or what if you want to focus on the seasonal component of unemployment by putting months on the x-axis and drawing one line for year? Those plots are difficult to do with this structure because each of the three variable is stored in a different way:

* `month` is stored in the rows
* `year` is spread across the columns
* `rate` is the value of each cell

To make it possible to plot this data we first need to tidy it. 

### Types of messiness {#sec:tidyr}

A tidy variable is a variable that's in a column; a messy variable is stored in some other way. There are three fundamental types of messy variable: column-spread, combined and row-spread. Any other types of messiness are just combinations of the three:

*   Col-spread: A pair of variables is spread across multiple columns. The 
    column names give the value one variable (the __key__), and the cell values 
    give the values of the other variable (the __value__).

    For example, the following dataset has three variables: month, year,
    and unemployment rate. Month is tidy, year and rate are spread across
    columns. Year gives the column name, and rate is the cell values.
    
    ```{r}
    ggplot2::economics %>% 
      tbl_df() %>%
      transmute(year = year(date), month = month(date), rate = uempmed) %>%
      filter(year > 2000) %>%
      spread(year, rate) %>% 
      head()
    ```

*   Combined: Multiple variables are stored in one column.

    ```{r, echo = FALSE}
    dplyr::data_frame(
      var = paste0(rep(c("start", "end"), each = 3), "_", rep(1:3, 2))
    )
    ```
    
    It's common to see a combination of row-spread and combine messiness 
    where the column names are a combination of multiple variables.
    
*   Row-spread: The rarest form of messiness is when a column contains the 
    variable names (the __key__) is paired with another column that gives the 
    values of the variable (the __value__).
    
    ```{r}
    dplyr::data_frame(
      obs = rep(c("temp", "rain"), each = 3),
      val = c(c(23, 22, 20), c(0, 0, 5))
    )
    ```

### Tidying messy variables

To tidy a messy data set, you first identify the variables, and the tidy them in the same order as above:

* Use `gather()` to combine col-spread variables into key and value 
  columns.

* Use `separate()` to turn combined variables into individual variables.
  
* Use `spread()` to turn a key-value pair of row-spread variables into 
  individual columns.

You always perform the operations (`spread()`, `separate()` and `gather()`) in the same order, but not every dataset requires all three operations. I haven't said anything about observations: if you focus on making sure each variable is in a column, then the observations will take care of themselves. (That said, once you've tidied the data it's worth thinking about exactly what a row of data represents.)

#### Spread

The inverse of spread is gather. So to tidy a 

We're going to use the __tidyr__ package. We're going to start with the `gather()` function. `gather()` takes variables that are spread across columns and values.

```{r}
ec3 <- 
  ec2 %>% 
  gather(year, rate, `2001`:`2007`, convert = TRUE, na.rm = TRUE)
```

We use `convert = TRUE` to automatically convert the years from character strings to numbers, and `na.rm = TRUE` to remove the months with no data. (In some sense the data isn't actually missing because it represents dates that haven't occured yet.)

Now we can easily visualise this to emphasise either the long term trend:

```{r}
ggplot(ec3, aes(year + (month - 1) / 12, rate)) +
  geom_line()
```

Or the seasonal patterns:

```{r}
ggplot(ec3, aes(month, rate, group = year)) +
  geom_line(aes(colour = year), size = 1)
```

## Data transformation with **dplyr** {#sec:dplyr}

There are three verbs for data transformation that are very useful:

### Filter observations

* `filter()`

### Create new variables

* `mutate()`

### Perform group-wise summarise

* `summarise()`

(Grouped mutates and filters are also useful, but more advanced. See the window function vignette for more details. )

### Fitting multiple models {#sub:multiple-models}

`do()`

In this section, we'll work through the process of generating the smoothed data produced by `stat_smooth()`.  This process will be the same for any other statistic, and should allow you to produce more complex summaries that `ggplot` can't produce by itself.  Figure~\ref{fig:smooth} shows the group-wise smoothes produced by the following code. \index{Model!fitting multiple models} \indexf{stat_smooth}

```{r smooth, fig.cap="A plot showing the smoothed trends for price vs. carat for each colour of diamonds. With the full range of carats (left), the standard errors balloon after around two carats because there are relatively few diamonds of that size. Restricting attention to diamonds of less than two carats (right) focuses on the region where we have plenty of data."}
qplot(carat, price, data = diamonds, geom = "smooth", 
  colour = color)
dense <- subset(diamonds, carat < 2)
qplot(carat, price, data = dense, geom = "smooth", 
  colour = color,  fullrange = TRUE)
```

How can we re-create this by hand?  First we read the `stat_smooth()` documentation to determine what the model is: for large data it's `gam(y ~ s(x, bs = "cs"))`.  To get the same output as `stat_smooth()`, we need to fit the model, then predict it on an evenly spaced grid of points. This task is performed by the `smooth()` function in the following code.  Once we have written this function it is straightforward to apply it to each diamond colour  using `ddply()`. \index{Package!mgcv}

Figure~\ref{fig:smooth-by-hand} shows the results of this work, which are identical to what we got with `ggplot` doing all the work.

```{r smooth-by-hand, fig.cap="Figure~\\ref{fig:smooth} with all statistical calculations performed by hand.  The predicted values (left), and with standard errors (right)."}
library(mgcv)
smooth <- function(df) {
  mod <- gam(price ~ s(carat, bs = "cs"), data = df)
  grid <- data.frame(carat = seq(0.2, 2, length = 50))
  pred <- predict(mod, grid, se = T)
  
  grid$price <- pred$fit
  grid$se <- pred$se.fit
  grid
}
smoothes <- dense %>% 
  group_by(color) %>%
  do(smooth(.))

qplot(carat, price, data = smoothes, colour = color, 
  geom = "line")
qplot(carat, price, data = smoothes, colour = color, 
  geom = "smooth", ymax = price + 2 * se, ymin = price - 2 * se)
```

Doing the summary by hand gives you much more flexibility to fit models where the grouping factor is explicitly included as a covariate. For example, the following model models price as a non-linear function of carat, plus a constant term for each colour. It's not a very good model as it predicts negative prices for small, poor-quality diamonds, but it's a starting point for a better model.

```{r gam, prompt=TRUE, fig.align='left'}
mod <- gam(price ~ s(carat, bs = "cs") + color, data = dense)
grid <- with(diamonds, expand.grid(
  carat = seq(0.2, 2, length = 50),
  color = levels(color)
))
grid$pred <- predict(mod, grid)
qplot(carat, pred, data = grid, colour = color, geom = "line")
```

See also [varying aesthetics and data](#sub:different-aesthetics) and [revealing uncertainty](#sec:uncertainty) for other ways of combining models and data.

### Other verbs

* `rename()`

* `transmute()`

There are two other verbs that are less useful for visualisation:

* `arrange()`, that can be useful when you're looking at the data from the 
  console. It doesn't affect visualisations because ggplot2 doesn't care about 
  the order of the rows. 
  
* `select()` picks variables based on their names. Useful when you have 
  very many variables and want to focus on just a few for analysis.

There are also verbs for working with two tables at a time.

## Visualising models with **broom** (#sec:broom)

(The previous version of the book discussed the `fortify()` generic. `fortify()` works with more than just models, but the most important uses of `fortify()` for non-model data now have better approaches, and **broom** wraps many more types of models that `fortify()`. It's also advantageous that it's a separate package because you can use it with other visualisation packages and it's easier to contribute to because the code is so much simpler. Currently broom development is very active.)

### Linear models

Currently, `ggplot` provides only one fortify method, for linear models. Here we'll show how this method works, and how you can use it to create tailored plots for better understanding your data and models. Figure~\ref{fig:plot-lm} shows the output of `plot.lm()` for a simple model. The graphics are a set of pre-chosen model summary plots. These are useful for particular problems, but are completely inflexible: there is no way to modify them apart from opening up the source code for `plot.lm()` and modifying it. This is hard because the data transformation and display are inextricably entangled, making the code difficult to understand. \index{Model!diagnostics} \index{Model!linear} \index{Linear models} \indexf{fortify.lm}

```{r plot-lm, out.width="0.4\\linewidth", fig.cap="The output from \\texttt{plot.lm()} for a simple model."}
mod <- lm(cty ~ displ, data = mpg)
plot(mod)
```

The `ggplot` approach completely separates data transformation and display. The `fortify()` method does the transformation, and then we use `ggplot` as usual to create the display that we want. Currently `fortify()` adds the variables listed in Table~\ref{tbl:fortify-vars} to the original dataset. These are basically all the variables that `plot.lm()` creates in order to produce its summary plots. The variables have a leading `.` (full stop) in their names, so there is little risk that they will clobber variables already in the dataset.

\begin{table}
  \centering
  \begin{tabular}{lp{2.5in}}
    \toprule
    Variable & Description \\
    \midrule
    \texttt{.cooksd}   & Cook's distances \\
    \texttt{.fitted}   & Fitted values \\
    \texttt{.hat}      & Diagonal of the hat matrix \\
    \texttt{.resid}    & Residuals \\
    \texttt{.sigma}    & Estimate of residual standard deviation when corresponding observation is dropped from model \\
    \texttt{.stdresid} & Standardised residuals \\
    \bottomrule
  \end{tabular}
  \caption{The diagnostic variables that \texttt{fortify.lm} assembles and adds to the model data.}
  \label{tbl:fortify-vars}
\end{table}

<!--
% If we just supply \f{fortify} with the model, it will add the diagnostic columns to the model data frame (which just contains the variables used in the model), or we can also supply the full original dataset.  
-->

To demonstrate these techniques, we're going to fit the very simple model with code below, which also creates the plot in Figure~\ref{fig:fortify-mod}. This model clearly doesn't fit the data well, so we should be able to use model diagnostics to figure out how to improve it. A sample of the output from fortifying this model is shown in Table~\ref{tbl:fortify-out}. Because we didn't supply the original data frame, it contains the two variables used in the model as well as the six diagnostic variables. It's easy to see exactly what data our plot will be working with and we could easily add more variables if we wanted.

```{r fortify-mod, fig.cap="A simple linear model that doesn't fit the data very well."}
qplot(displ, cty, data = mpg) + geom_smooth(method = "lm")
mpgmod <- lm(cty ~ displ, data = mpg)
```

```{r fortify-out, echo=FALSE, results='hide', eval=FALSE}
xtable(head(fortify(mpgmod)), caption = "The output of \\texttt{fortify(mpgmod)} contains the two variables used in the model (\\texttt{cty} and \\texttt{displ}), and the six diagnostic variables described above.", label = "fortify-out")
```

<!--
% You may notice some similarity between this approach and the transformations performed by stats.  The major difference is that \f{fortify} is global, while statistical transformations are local to the facet and group.
-->

With a fortified dataset in hand we can easily re-create the plots produced by `plot.lm()`, and even better, we can adapt them to our needs. The example below shows how we can re-create and then extend the first plot produced by `plot.lm()`. Once we have the basic plot we can easily enhance it: use standardised residuals instead of raw residuals, or make size proportional to Cook's distance. The results are shown in Figure~\ref{fig:fortify-fr}.

```{r fortify-fr, out.width="0.32\\linewidth", fig.cap="(Left) Basic fitted values-residual plot. (Middle) With standardised residuals. (Right) With size proportional to Cook's distance. It is easy to modify the basic plots when we have access to all of the data."}
mod <- lm(cty ~ displ, data = mpg)
basic <- ggplot(mod, aes(.fitted, .resid)) +
  geom_hline(yintercept = 0, colour = "grey50", size = 0.5) + 
  geom_point() + 
  geom_smooth(size = 0.5, se = F)
basic
basic + aes(y = .stdresid)
basic + aes(size = .cooksd) + scale_size_area("Cook's distance")
```

Additionally, we can fortify the whole dataset and add to the plot variables that are in the original data but not in the model. This helps us to understand what variables are useful to improve the model. Figure~\ref{fig:fortify-full} colours the residuals by the number of cylinders, and suggests that this variable would be good to add to the model: within each cylinder group, the pattern is close to linear.

```{r fortify-full, fig.cap="Adding variables from the original data can be enlightening. Here when we add the number of cylinders we see that instead of a curvi-linear relationship between displacement and city mpg, it is essentially linear, conditional on the number of cylinders."}
full <- basic %+% fortify(mod, mpg)
full + aes(colour = factor(cyl))
full + aes(displ, colour = factor(cyl))
```
