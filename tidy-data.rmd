---
title: Tidy data
output: bookdown::html_chapter
bibliography: references.bib
---

```{r data, include = FALSE}
chapter <- "tidy_data"
source("common.R")
library(tidyr)
```

# Data analysis {#cha:data}

So far, every example in this book has started with a nice dataset that's easy to plot. That's great for learning (because you don't want to struggle with getting data handling while you're learning visualisation) but in real-life data hardly ever comes in exactly the right sturcture. To use ggplot2 in practice, you also need to learn some data wrangling skills. Indeed, in my experience, visualisation is often the easiest part of the data analysis process: once you have the right data, in the right format, aggregated in the right way, the right visualisation is often obvious.

The goal of this part of the book is to show you how to integrate ggplot2 with other tools needed for a complete data analysis:

* In this chapter, you'll learn the principles of tidy data, which help you
  organise your data in a way that makes it easy for ggplot2 to work with. 
  This chapter will teach you how to use __tidyr__ to tidy messy dataset into
  a form that's easier to work with.
  
* Most visualisations also require some data transformation whether it's 
  creating a new variable from existing variables, or performing simple 
  aggregations so you can see the forest for the tree. Chapter XXX will teach 
  how to use the __dplyr__ package to make this as easy as possible.
  
* If you're using R, you're almost certainly using it for it's fantastic
  modelling capabilities. While there's an R package for almost every type
  of model that you can think of, the results of these models can be hard to
  visualise. In Chapter YYY, you'll learn about the __broom__ package, by David 
  Robinson, which makes your life easier by converting model outputs into 
  standard tidy datasets so you can easily integrate them with ggplot2.

Tidy data is the foundation for both data manipulation and visualising models. In the following sections, you'll learn the precise definition of tidy data, and the tools you need to make messy data tidy. The chapter concludes with two case studies that show how to apply the tools in sequence to work with real(istic) data.

## Tidy data {#sec:tidy-data}

The principle behind tidy data is simple: storing your data in a consistent way makes it easier to work with it. Tidy data is a mapping between statistical structure of a data frame (variables and observations) with the physical structure (columns and rows). Tidy data follows two main principles:

1. Variables go in columns.
1. Observations go in rows.

If you store data in this way, you'll find most data analysis tasks become easier.

Tidy data is particularly important for ggplot2 because the job of ggplot2 is to map variables to visual properties. ggplot2 assumes that variables are stored in columns, so if your data isn't tidy, you'll have a hard time visualising it. Sometimes you'll find a dataset that you have no idea how to plot. That's normally because it's not tidy: the variables aren't provided as columns, so you can't tell ggplot2 what to plot. For example, take this data frame that contains monthly employment data for the United States:

```{r, echo = FALSE, message = FALSE}
library("lubridate")
ec2 <- 
  ggplot2::economics %>% 
  tbl_df() %>%
  transmute(year = year(date), month = month(date), rate = uempmed) %>%
  filter(year > 2000) %>%
  spread(year, rate)
knitr::kable(ec2)
```

(If it looks familiar it's because it's dervied from the `economics` dataset that you saw earlier.)

Imagine you want to plot a time series showing how unemployment has changed over the last 10 years. Can you picture the ggplot2 command you'd need to do it? What if you wanted to focus on the seasonal component of unemployment by putting months on the x-axis and drawing one line for each year? It's difficult to see how to create those plots because the data is not tidy. There are three variables, month, year and unemployment rate, but each variable is stored in a different way:

* `month` is stored in one column.
* `year` is spread across the first row.
* `rate` is the value of each cell.

To make it possible to plot this data we first need to tidy it. There are two important pairs of tools: 

* Spread & gather.
* Separate & unite.

## Spread and gather {#sec:spread-gather}

Take a look at the two tables below:

```{r, echo=FALSE}
indexed <- data.frame(
  w = c(1, 2, 3, 4, 3),
  x = c("a", "b", "c", "d", "c"),
  y = c("A", "D", "A", "C", "B"),
  z = c(1, 5, 4, 9, 10)
) %>% arrange(x, y)
matrix <- indexed %>% spread(y, z)

knitr::kable(indexed)
knitr::kable(matrix)
```

If you study them for a little while, you'll notice that they contain the same data, but in a different form. The first form is called __indexed__ data, because you look up a value using an index. The second form is called __Cartesian__ form, because it's you find a value by looking for intersection of a row and a column.

(Note that we can't tell if these datasets are tidy or not. Either form could be tidying depending on what the values "A", "B", "C", "D" mean. Also note the missing values: missing values that are explicit in one form, maybe implicit in the other. An `NA` is the presence of an absense; but sometimes a missing value is the absense of a presence.)

Transforming from one form to the other always requires a pair: a _key_ and a _value_. When spreading, going from indexed to matrix forms, the key is the column that will become column names, and the value is the column that fills in the values. When gathering, going from matrix to indexed, the key is the name of the column that will be created from the row names, and the value is the name of the column that will be created from the cell values. To help you remember the verbs, generally:

* gather reduces the number of columns: n -> 2.
* spread increases the number of columns: 2 -> n.

(You could imagine generalising spread and gather to higher dimensions. However, data is almost always stored in 2d (rows & columns), so these generalisations, while fun to think about, are not that practical. @wickham:2007b explores this idea in more detail.)

Don't worry if this seems a bit confusing at first. You'll get the hang of it as you work through some concrete examples.

The tidyr package provides the `spread()` and `gather()` functions to perform these operations. The first three arguments to `spread()` and `gather()` are the same:

* `data`: the dataset to modify
* `key` & `value`: the key-value pair

`gather()` has two additional arguments:

* `...` allows you to specify gather up. You can specify individually, 
   `A, B, C, D`, or as a range `A:D`. Alternatively, you can specify which
   columns are _not_ to be gather with `-`:  `-E, -F`.
   
* `na.rm`, if `TRUE` this will convert explicit missing values in matrix
  form into _implicit_ missing values in indexed form. In other words, 
  `na.rm = TRUE` makes missing values go missing.

To convert between the two forms of sample data above:

```{r}
library(tidyr)
spread(indexed, key = y, value = z)
gather(matrix, key = y, value = z, A:D, na.rm = TRUE)
```

To tidy the economics dataset shown above, you first need to identify the variables. Note that there's a pair of variables that are stored in the column names and in the cell values. This data is in matrix form and we need to put it in indexed form to make it tidy. In this example, the key is `year`, the value is `unemp` and we want to select columns from `2001` to `2007`:

```{r}
gather(ec2, key = year, value = unemp, `2001`:`2007`)
```

Note that the columns have names that are not standard varible names in R (they don't start with a letter). This means that we need to surround them in backticks, `` `2001` `` to refer to them.

To be useful, we need two extra args:

```{r}
economics <- gather(ec2, year, rate, `2001`:`2007`, convert = TRUE, na.rm = TRUE)
```

We use `convert = TRUE` to automatically convert the years from character strings to numbers, and `na.rm = TRUE` to remove the months with no data. (In some sense the data isn't actually missing because it represents dates that haven't occured yet.)

When the data is in this form, it's easy to visualise in many different ways. We can emphasise the long term trend:

```{r}
ggplot(economics, aes(year + (month - 1) / 12, rate)) +
  geom_line()
```

Or the seasonal patterns:

```{r}
ggplot(economics, aes(month, rate, group = year)) +
  geom_line(aes(colour = year), size = 1)
```

Spreading is the opposite of gathering. You use it when you have a pair of columns that contain the name of a variable and its value. The following example dataset contains three variables (day, rain and temp), but rain and temp are stored in the obs-val pair. Spread allows us to turn the indexed form into a tidy matrix form:

```{r}
weather <- dplyr::data_frame(
  day = rep(1:3, 2),
  obs = rep(c("temp", "rain"), each = 3),
  val = c(c(23, 22, 20), c(0, 0, 5))
)
spread(weather, key = obs, value = val)
```

This form of data is less common, but it does crop up.

## Separate and unite {#sec:separate-unite}

Spread and gather help when the variables are in the wrong place in the dataset. Separate and unite help when multiple variables are crammed into one column, or spread across multiple columns. 

For example, the following dataset stores some information about the response to a medical treatment. There are three variables (time, treatment and value), but time and treatment are jammed in one variable together: 

```{r, echo = FALSE}
trt <- dplyr::data_frame(
  var = paste0(rep(c("start", "end"), each = 3), "_", rep(c("a", "b", "c"), 2)),
  val = c(1, 4, 2, 10, 5, 11)
)
```

The `separate()` function makes it easy to tease apart multiple variables stored in one column. It takes four arguments:

* `data`: the data frame to modify
* `col`: the name of the variable to split into pieces
* `into`: a character vector giving the names of the new variables
* `sep`: a description of how to split the variable apart. This can either be
  a regular expression, e.g. `_` to split by underscores, or `[^a-z]` to split 
  by any non-letter, or an integer giving a position.
  
```{r}
separate(trt, var, c("time", "treatment"), "_")
```

(If the variables are combined in a more complex form, have a look at `extract()` as a more complex form of `separate()`. Alternatively, you might need to create columns individually yourself using other calculations. A useful tool for this is `mutate()` which you'll learn about in the next chapter.)

`unite()` is the inverse of `separate()` - it joins together multiple columns into one column. This is much common, but it's important to recognise that tidying verbs always come in paris.

## Case studies {#sec:tidy-case-study}

For most real datasets, you'll need to use more than one tidying verb. There many be multiple ways to get there, but as long as each step makes the data tidier, you'll eventually get to a tidy dataset. However, as a general rule, typically you apply the functions in the same order: you might not use all of them, but most of time you use them in order of `gather()`, `separate()` and `tidy()`.

### Blood pressure

The first step when tidying a new dataset is always to identify the variables. Take the following simulated medical data. There are seven variable in this dataset: name, age, start date, week, systolic & diastolic blood pressure. Can you see how they're stored?

````{r}
bpd <- readr::read_table("name age      start  week1  week2  week3
Anne  35 2014-03-27 100/80 100/75 120/90
 Ben  41 2014-03-09 110/65 100/65 135/70
Carl  33 2014-04-02 125/80   <NA>   <NA>
", na = "<NA>")
```

The first step is to gather columns that aren't variables into a key-value (week-bp) pair:

```{r}
bpd_1 <- gather(bpd, week, bp, week1:week3)
bpd_1
```

This is tidier, but we have two variables combined together in the `bp` variable. This is a common way of writing down the blood pressure, but analysis is easier if we break into two variables.  That's the job of separate:

```{r}
bpd_2 <- separate(bpd_1, bp, c("sys", "dia"), "/", extra = "drop")
bpd_2
```

This dataset is now tidy, but we could do a little more to make it easier to use. The following code uses extract to pull the week number out into its own variable (using regular expressions is beyond the scope of the book, but `\\d` stands for any digit). I also use arrange (which you'll learn about in the next chapter) to order the rows to keep the records for each person together.

```{r}
bpd_3 <- extract(bpd_2, week, "week", "(\\d)", convert = TRUE)
bpd_4 <- arrange(bpd_3, name, start)
bpd_4
```

You might notice that there's some repetition in this dataset: if you know the name, then you also know the age and start date. This reflects a third condition of tidyness that I don't discuss here: each data frame should contain one and only one data set. Here there are really two datasets: information about each person that doesn't change over time, and their weekly blood pressure measurements. You can learn more about this sort of messiness in the resources mentioned at the end of the chapter.

### Test scores

Imagine you're intersted in the effect of an intervention on test scores. You've collected the folllowing data. What are the variables?

```{r}
# Adapted from http://stackoverflow.com/questions/29775461
scores <- data_frame(
  person = rep(c("Greg", "Sally", "Sue"), each = 2),
  time   = rep(c("pre", "post"), 3),
  test1  = round(rnorm(6, mean = 80, sd = 4), 0),
  test2  = round(jitter(test1, 15), 0)
)
```

The variables are person, test, pre-test score and post-test score. As usual, we start by converting columns in matrix form (test1 and test2) in to indexed form (test and score):
```{r}
scores_1 <- gather(scores, test, score, test1:test2)
scores_1
```

Now we need to do the opposite: pre and post should be variables, so we need to spread time and value:

```{r}
scores_2 <- spread(scores_1, time, score)
scores_2
```

A good indication that we have done this correctly is that it's now easier to calculate the statistic of interest: the difference between pre- and post-intervention scores:

```{r}
mutate(scores_2, diff = post - pre)
```

(Again, you'll learn about `mutate()` in the next chapter.)

## Learning more

Data cleaning, manipulation and transformation is a big topic and this book only scratches the surface. I recommend the following references which go into considerably more depth on this topic:

* The documentation. I'll describe the most important arguments, but most
  functions have other arguments that help deal with less common situations.
  If you're struggling, make sure to read the documentation to see if there's
  an argument that might help you.

* "[Tidy data](http://www.jstatsoft.org/v59/i10/)", an article in the _Journal
  of Statistical Software_. It describes the ideas of tidy data in more depth
  and shows other types of messy data. Unfortunately the paper was written
  before tidyr existed, so to see how to use tidyr instead of reshape2, consult
  the 
  [tidyr vignette](http://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html).

## References