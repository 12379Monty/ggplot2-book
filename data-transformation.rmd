---
title: Data transformation
output: bookdown::html_chapter
bibliography: references.bib
---

```{r data, echo = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
options(digits = 2, width = 60, dplyr.print_min = 5, dplyr.print_max = 5)
knitr::opts_chunk$set(comment = "#>", compact = TRUE)

diamonds <- diamonds %>% tbl_df()
```

# Data transformation {#cha:dplyr}

During the course of creating a visualisation you'll often discover that data doesn't quite have the variables you need, or maybe it's aggregated in a slightly awkward way. To go along with your visualistion skills, you also need some basic data manipulation skills. For this data manipulation problems, I recommend learning `dplyr()` which is designed in a very similar way to ggplot2: it works with tidy data.

The goal of dplyr is to provide verbs (functions) that help you solve the most common 95% of data manipulation problems. In many ways, dplyr is similar to ggplot2, but instead of providing a grammar of graphics, it provides a grammar of data manipulation. Like ggplot2, dplyr helps you not only but providing functions, but by providing a useful way for you to think about data manipulation. In particular, dplyr helps by constraining you: instead of struggling to think about which of the thousands of functiosn that might help, you can just pick from a handful that are design to be very likely to be helpful.

In this chapter you'll learn four of the most important dplyr verbs:

* `filter()`
* `mutate()`
* `group_by()`
* `summarise()`

These verbs are easy to learn because they all work the same way: they take a data frame as the first argument, and return a modified data frame. The second and subsequent arguments control the details of the transformation, and are always interpreted in the context of the data frame so you can refer to variables directly. I'll also explain each in the same way: I'll show you a motivating examples using the diamonds, and then give you more details about how the function works, and finish up with some exercises for you to practice your skills with.

You'll also learn how to create data transformation pipelines using `%>%`. `%>%` plays a similar role to `+` in ggplot2: it allows you to solve complex problems by combining small pieces that are easily understood in isolation.
  
This only scratches the surface of dplyr's capabilities but should be enough to get help you do visualisation. The chapter concludes with some pointers to more resources to help you master dplyr for other data manipulation needs.

## Filter observations

It's common to only want to explore one part of a dataset. A great data analysis strategy is to start with just one obseravation unit (one person, one city, etc), and understand how it works before attempting to generalise the conclusion to others. If you ever feel overwhelmed by an analysis this is great technique: zoom down to something that's not overwhelming, master it, and then zoom back out.

Filtering is also useful for extracting outliers. Generally, you don't want to just throw outliers away (often they are were the action is), but it's useful to think about partioning the data into the common and the unusual. You summarise the common to look at the broad trends; you examine the outliers individually to see if you can figure out what's going on.

For example, look at this plot that shows how the x and y dimensions of the diamonds are related:

```{r, dev = "png", fig.show = "hold", fig.width = 3, fig.height = 3}
qplot(x, y, data = diamonds) + theme(aspect.ratio = 1)
```

There are around 50,000 points in this dataset: most of them lie along the diagonal, but there are a handful of outliers. One clear set of incorrect values are those diamonds with zero dimensions. We can use `filter()` to pull them out.

```{r}
filter(diamonds, x == 0 | y == 0)
```

This is equivalent to the base R code `diamonds[diamonds$x == 0 | diamonds$y == 0, ]`, but is less repitious because `filter()` allows you to refer to bare variable names. 

(If you've used `subset()` before, you'll notice that it has very similar behaviour. The biggest difference is that `subset()` can select both observations and variables, where in dplyr, `filter()` works with exclusuively with observations and `select()` with variables. There are some other subtle differences, but the main advantage to using `filter()` is that it behaves identically to the other dplyr verbs)

Once we've looked at these outliers, we probably want on the clearly good values so we can more clearly see the pattern. To save a little typing, we can take advantage of the fact that multiple arguments to `filter()` are combined with and.

```{r, dev = "png"}
diamonds_ok <- filter(diamonds, x > 0, y > 0, y < 20)
qplot(x, y, data = diamonds_ok) + 
  theme(aspect.ratio = 1) + 
  geom_abline(slope = 1, colour = "red")
```

This plot is now more informative - we can see a very strong relationship between x and y (and the reference abline helps us see exactly what that relationship is). However, this plot still has problems: 

* Most of the plot region is empty, because most of the data lies along the 
  diagonal. 
  
* There are some clear bivariate outliers, but it's hard to select those
  outliers with a simple filter need a non-obvious function of x and y. 
  
We'll solve both of these problem in the next section by adding a new variable that's a transformation of x and y. But before we continue on to that, lets talk more about the details of `filter()`.

### Useful tools

The two most important arguments to `filter()` a data frame and an expression that creates a logical vector. The logical vector must always be the same length as the data frame: if not, you'll get an error. Typically you create the logical vector with the comparison operators:

* `x == y`: x and y are equal.
* `x != y`: x and y are not equal.
* `x %in% c("a", "b", "c")`: x is one of the values in the RHS.
* `x > y`, `x >= y`, `x < y`, `x <= y`: greater than, greater than or equal to,
  less than, less than or equal to.

There are three possible values in a logical vector: TRUE, FALSE, or NA. `filter()` keeps the observations corresponding to TRUE values (i.e. both FALSE and NA values are dropped). Change this behaviour by modifying the logical vector:

* `!x` (pronounced not x), flips TRUE and FALSE so it keeps all the values 
  where `x` is FALSE.
  
* `is.na(x)` is TRUE when x is NA, and FALSE otherwise, so it keeps all the 
  values where `x` is NA. 

Use logical operators to combine the results of multiple expressions:

* `x & y`: TRUE if both x and y are TRUE.
* `x | y`: TRUE if either x or y are TRUE.
* `xor(x, y)`: TRUE if either x or y are TRUE, but not both (e__x__clusive or).

Most real queries require some combination of comparison and logical operators:

* Price less than $500: `price < 500`
* Size between 1 and 2 carats: `carat >=1 & carat < 2`
* Cut is ideal or premium: `cut == "Premium" | cut == "Ideal"`, or 
  `cut %in% c("Premium", "Ideal")` (note that R is case sensitive)
* Worst colour, cut and clarify: `cut == "Fair" & color == "J" & clarity == "SI2"`
  
You can also use functions in the filtering expression:

* Size is between 1 and 2 carats: `floor(carat) == 1`
* An average dimension greater than 3: `(x + y + z) > 3` 

This is useful for simple expressions, but as things get more complicated it's better to create a new variable first so you can check that you've done the computation correctly before doing the subsetting. You'll learn how to do that in the next section.

The rules for NA are a bit trickier, so I'll explain them next.

### Missing values

NA, R's missing value indicator, can be frustrating to work with. R's underlying philosophy is to force you to recognise that you have missing values, and make a deliberate choice to deal with them: missing values never silently go missing. This is a pain because you almost always want to just get rid of them, but it's a good principle to force you to think about the correct option.  

The most important thing to understand about missing values is that they are infectious: with few exceptions, the result of any operation that includes a missing value will be a missing value. This happens because NA represents an unknown value, and there are few operations that turn an unknown value into a known value.NA 

```{r}
x <- c(1, NA, 2)
x == 1
x > 2
x + 10
```

When you first learn R, you might be tempted to find missing values using `==`:

```{r}
x == NA
x != NA
```

But that doesn't work! A little thought reveals why: there's no reason why unknown values should be the same. Instead, you use `is.na(X)` to determine if a value is missing:

```{r}
is.na(x)
```

`filter()` only includes observations where the logical is TRUE, so NA values are automatically dropped. If you want to include missing values, be explicit: `x > 10 | is.na(x)`. In other parts of R, you'll sometimes need to convert missing values into FALSE. You can do that with `x > 10 & !is.na(x)`

### Exercises

1.  Practice your filtering skills by:

    * Finding all the diamonds with equal x and y dimensions.
    * A depth between 55 and 70.
    * A carat smaller than the median carat.
    * Cost more than $10,000 per carat
    * Are of good or better quality

1.  Complete the missing pieces in this table:

    Expression       | TRUE   | FALSE | NA 
    -----------------|--------|-------|------
    `x`              | ✓      |       | 
    ?                |        | ✓     | 
    `is.na(x)`       |        |       | ✓
    `!is.na(x)`      | ?      | ?     | ?
    ?                | ✓      |       | ✓
    ?                |        | ✓     | ✓ 

1.  Repeat the analysis of outlying values to include the z dimension. Compared
    to x and y, how would you characterise the relationship of x and z, or y and 
    z?

1.  Look at the movies that have a missing budget. How are they different
    from the movies with a budget? (Hint: try a frequency polygon plus 
    `colour = is.na(budget)`.)

1.  What is `NA & FALSE` and `NA | TRUE`? Why? Why doesn't `NA * 0` equal zero? 
    What number times zero does not equal 0? What do you expect `NA ^ 0` to 
    equal? Why?

### Create new variables {#mutate}

To better explore the relationship between x and y, it's useful to "rotate" the plot so that the data is flat, not diagonal. We can do that by creating two new variables: one that represents the difference between x and y (which in this context represents the symmetry of the diamond) and one that represents its size (the length of the diagonal).

To create new variables we can use `mutate()`. Like `filter()` it takes a data frame as its first argument and returns a data frame. It's second and subsequent arguments are named expressions that generate new variables. Like `filter()` you can refer to variables just by their name, you don't need to also include the name of the dataset.

```{r, dev = "png"}
diamonds_ok <- mutate(diamonds_ok,
  sym = x - y,
  size = sqrt(x ^ 2 + y ^ 2)
)

diamonds_ok
qplot(size, sym, data = diamonds_ok)
```

This plot has two advantages: we can more easily see the pattern that the bulk on the points lie on, and we can easily select outliers. Here, it doesn't seem important whether the outliers are positive (i.e. x is bigger than y) or negative (i.e. y is bigger x). So we can use the absolute value of the symmetry variable to pull out the outliers. 0.25 seems a reasonable threshold. We'll check out the results with a histogram.

```{r}
qplot(size, abs(sym), data = diamonds_ok)
diamonds_ok2 <- filter(diamonds_ok, abs(sym) < 0.25)
qplot(sym, data = diamonds_ok2, binwidth = 0.01) 
```

That's an interesting histogram! While most diamonds are close to being symmetric there are very few that are perfectly symmetric (i.e. `x == y`.)

### Useful tools

Typically, the transformations will be suggested by your domain knowledge. However, there are a number of transformations that are useful in a surprisingly wide domain range of circumstances. 

* Log-transformations are often useful. They additive relationships into 
  multiplicative relationships; they compress data that varies over orders of 
  magnitude; they conver power relationships to linear relationship.
  See examples at <http://stats.stackexchange.com/questions/27951>

* Relative difference: If you're interested in the relative difference between 
  two variables, use `log(x / y)`. It's better than `x / y` because it's 
  symmetric: if x < y, `x / y` takes values [0, 1), but if x > y, `x / y` takes
  values (1, Inf). See @tornqvist:1985 for more details.
  
* Sometimes integrating of differentiating might make the data more 
  interpretable: if you have distance and time, would speed or acceleration be 
  more useful? (or vice versa). (Note that integration makes data more 
  smooth; differentiation makes it less smooth.)

* Partition a number into magnitude and direction with `abs(x)` and `sign(x)`.

There are also a few useful ways to transform pairs of variables:

* Partitioning into overall size and difference is often useful, as seen
  above.

* If you see a strong trend, use a model to partition it into pattern and 
  residuals is often useful. You'll learn more about that in the next chapter.
  
* Sometimes its useful to change positions to polar coordinates: distance 
  (`sqrt(x^2 + y^2)`) and angle (`atan2(y, x)`).

### Exercises

1. Practice your variable creation skills by creating new variables

    * The approximate volume of the diamond (using x, y, and z).
    * The approximate density of the diamond.
    * The price per carat.
    * Log transformation of carat and price.
    
1.  How can you improve the data density of `qplot(x, z, data = diamonds)`.
    What transformation makes it easier to extract outliers?

1.  The depth variable is just the width of the diamond (average of x and y)
    divided by its height (z) multiplied by 100 and round to the nearest 
    integer. Compute the depth yourself and compare it to the existing depth
    variable. Summarise your findings with a plot.

1.  Compare the distribution of diamond symmetry based for diamonds where 
    $x > y$ vs $y < x$.

### Group-wise summaries

Many insightful visualisations require that you reduce the full dataset down to a meaningful summary. ggplot2 provides a number of geoms that will do summaries for you. But it's often useful to do summaries by hand because you have more flexibility and you can use the summaries for purposes other than visualisation.

Dplyr does summaries in two steps:

1. Define the grouping variables with `group_by()`.
2. Describe how to summarise each group with a single row with `summarise()`

For example, to look at the average price per clarity, we first group by clarity, then summarise 

```{r, fig.height = 2}
by_clarity <- group_by(diamonds, clarity)
sum_clarity <- summarise(by_clarity, price = mean(price))
sum_clarity

ggplot(sum_clarity, aes(clarity, price)) + 
  geom_line(aes(group = 1), colour = "grey80") +
  geom_point(size = 4)
```

You might be surprised by this pattern - diamonds with better clarity tend to have lower average prices. We'll see why this is the case and what to do about it in the next chapter.

We can form groups with multiple variables by supplying additional variables to `group_by()`. The next example shows how we can compute a frequency polygon that shows how cut and depth interact by hand. The special summary function `n()` counts the number of observations in each group.

```{r, fig.height = 2}
cut_depth <- summarise(group_by(diamonds, cut, depth), n = n())
cut_depth <- filter(cut_depth, depth > 55, depth < 70)
cut_depth

ggplot(cut_depth, aes(depth, n, colour = cut)) + 
  geom_line()
```

We can use a grouped `mutate()` to convert counts to proportions, so it's easier to compare across the cuts. `summarise()` strips one level of grouping off, so `cut_depth()` will be grouped by cut.

```{r, fig.height = 2}
cut_depth <- mutate(cut_depth, prop = n / sum(n))
ggplot(cut_depth, aes(depth, prop, colour = cut)) + 
  geom_line()
```

### Useful tools

(Grouped mutates and filters are also useful, but more advanced. See the window function vignette for more details. )

Useful summary functions:

* Count: `n()`, `n_distinct(x)`.
* Middle: `mean(x)`, `median(x)`.
* Spread: `sd(x)`, `mad(x)`, `IQR(x)`.
* Extremes: `quartile(x)`, `min(x)`, `max(x)`.
* Position: `first(x)`, `last(x)`, `nth(x, 2)`.

Another extremely useful technique is to use `sum()` or `mean()` with logical vectors. When logical vectors are treated as numeric, TRUE becomes 1 and FALSE becomes 0. This means that `sum()` tells you the number of true elements, and `mean()` tells you the proportion of true elements. For example, the following code counts the number of diamonds with carat greater than or equal to 4, and the proportion of diamonds that cost less than $1000.

```{r}
summarise(diamonds, 
  n_big = sum(carat >= 4), 
  prop_cheap = mean(price < 1000)
)
```

Most summary functions have a `na.rm` argument: `na.rm = TRUE` tells the summary function to remove any missing values prior to summiarisation. This is a convenient shortcut: rather than removing the missing values then summarising, you can do it in one step.

### Statistical considerations

When summarising with the mean or median, it's always a good idea to include a count and a measure of spread. This helps you calibrate your assessments - if you don't include them you're likely to think that the data is less variable than it really is, and potentially draw unwarranted conclusions.

```{r}
by_cut <- diamonds %>%
  group_by(color) %>%
  summarise(
    n = n(), 
    med = as.numeric(median(price)), 
    lq = quantile(price, 0.25), 
    uq = quantile(price, 0.75)
  )
by_cut
ggplot(by_cut, aes(color, med, group = 1)) + 
  geom_linerange(aes(ymin = lq, ymax = uq), colour = "grey50") + 
  geom_point(aes(size = n)) + 
  geom_line() + 
  scale_size_area()
```

Another example of this comes from baseball. Let's take the MLB batting data from the Lahman package and calculate the batting average, the number of hits divded by the number of at bats. This is the proportion that a batter hits the ball. Who's the best batter according to this metric?

```{r}
data(Batting, package = "Lahman")
batters <- filter(Batting, AB > 0)
per_player <- group_by(batters, playerID)
ba <- summarise(per_player, ba = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE))
ggplot(ba, aes(ba)) + 
  geom_histogram()
```

Wow, there are a lot of players who can hit the ball every single time! Would you want them on your fantasy baseball team?  Let's double check they're really that good by calibrating also showing the total number of at bats:

```{r, dev = "png"}
ba <- summarise(per_player, 
  ba = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),
  ab = sum(AB))
ggplot(ba, aes(ab, ba)) + 
  geom_point(alpha = 1/5) + 
  geom_smooth()
```

The highest batting averages occur for the players with the smallest number of at bats - it's not hard to hit the ball every time if you've only had two pitches.  We can make the pattern a little more clear by getting rid of the players with less than 10 at bats.

```{r, dev = "png"}
ggplot(filter(ba, ab >= 10), aes(ab, ba)) + 
  geom_point(alpha = 1/5) + 
  geom_smooth()
```

You'll often see a similar pattern whenever you plot number of observations vs. an average. 
This shows a typical conical pattern - our estimate of average runs per at-bat is very noisy when we have few data points, and less noisy when we have more. There's also a moderate increasing trend here - as the number of games increases, so too appears the average 

### Exercises

1.  For each year in the movies data, determine the percent of movies with 
    missing budgets. Visualise the result.
    
1.  How does the average length of a movie change over time? Display your 
    answer with a plot, including some display of uncertainty.
    
1.  For each combination of diamond quality (e.g. cut, colour and clarity), 
    count the number of diamonds, the average price and the average size. 
    Visualise the results.

1.  Compute a histogram of carat by "hand". Use a binwidth of 0.1. 
    (Hint: you might need to create a new variable first).

## Transformation pipelines

In only the simplest of cases will you apply just one dplyr verbs. In most real analyses, you'll end up stringing together multiple `mutate()`s, `filter()`s, and `group_by()`s `summarise()`s. For example, above, we created a frequency polygon by hand by with a combination of all four verbs:

```{r, results = "none"}
# By using intermediate values
cut_depth <- group_by(diamonds, cut, depth)
cut_depth <- summarise(cut_depth, n = n())
cut_depth <- filter(cut_depth, depth > 55, depth < 70)
cut_depth <- mutate(cut_depth, prop = n / sum(n))
```

This sequence of operations is a bit painful because we repeated the name of the data frame many times. An alternative is just to do it with one sequence of function calls:

```{r, results = "none"}
# By "composing" functions
mutate(
  filter(
    summarise(
      group_by(
        diamonds, 
        cut, 
        depth
      ), 
      n = n()
    ), 
    depth > 55, 
    depth < 70
  ), 
  prop = n / sum(n)
)
```

But this is also hard to read because the sequence of operations is inside out, and the arguments to each function can be quite far apart.  dplyr provides a better approach with the __pipe__, `%>%`. With the pipe, we can write the above sequence of operations as:

```{r, results = "none"}
cut_depth <- diamonds %>% 
  group_by(cut, depth) %>% 
  summarise(n = n()) %>% 
  filter(depth > 55, depth < 70) %>% 
  mutate(prop = n / sum(n))
```

This makes it easier to understand what's going on as we can read it almost like an English sentence: first group, then summarise, then filter, then mutate. In fact, the best way to pronounce `%>%` when reading a sequence of code is as "then".

`%>%` works by taking the thing on the left hand side (LHS) and supplying it as the first argument to the function on the righ hand side (RHS). Each of these pairs of calls is equivalent:

```{r, eval = FALSE}
f(x, y)
x %>% f(y)

g(f(x, y), z)
x %>% f(y) %>% g(z)
```

### Exercises

1.  Translate the following code to use the pipe. Describe what it does in 
    English.

1.  What does the following pipe do?

    ```{r}
    library(magrittr)
    x <- runif(100)
    x %>%
      subtract(mean(.)) %>%
      raise_to_power(2) %>%
      mean() %>%
      sqrt()
    ```

1.  Convert each of these examples to either use the pipe or to use function 
    composition:
    
    *
    *
    *
    *

1.  Data analysis challenge.

1.  Data analysis challenge.

## Learning more

There are two other verbs that are less useful for visualisation:

* `arrange()`, that can be useful when you're looking at the data from the 
  console. It doesn't affect visualisations because ggplot2 doesn't care about 
  the order of the rows. 
  
* `select()` picks variables based on their names. Useful when you have 
  very many variables and want to focus on just a few for analysis.

* `rename()` allows you to change the name of variables.

This has given you the basics for doing single table data manip in R. Learn more:

* Cheatsheet

* The verbs that work with two tables at a time: mutating joins, filtering
  joins and the set operations.
  
* Dplyr can work directly with data stored in a database - you use the same
  R code as you do for local data and dplyr generates SQL to send to the
  database.

* Group-wise filters and mutate can also be useful, particularly in
  conjunction with window functions.

* The dplyr vignettes, which you can see with 
  `browseVignettes(package = "dplyr")`, go into considerably more depth into
  dplyr. There you'll learn how to also use dplyr with data frames, and how
  to work with multiple tables of data.

