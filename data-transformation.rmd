---
title: Data transformation
output: bookdown::html_chapter
bibliography: references.bib
---

```{r data, echo = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
options(digits = 2, width = 60)
knitr::opts_chunk$set(comment = "#>", compact = TRUE)
diamonds <- diamonds %>% tbl_df()
```

# Data transformation {#sec:dplyr}

During the course of creating a visualisation you'll often discover that data doesn't quite have the variables you need, or maybe it's aggregated in a slightly awkward way. To go along with your visualistion skills, you also need some basic data manipulation skills. For this data manipulation problems, I recommend learning `dplyr()` which is designed in a very similar way to ggplot2: it works with tidy data.

The goal of dplyr is to provide verbs (functions) that help you solve the most common 95% of data manipulation problems. In many ways, dplyr is similar to ggplot2, but instead of providing a grammar of graphics, it provides a grammar of data manipulation. Like ggplot2, dplyr helps you not only but providing functions, but by providing a useful way for you to think about data manipulation. In particular, dplyr helps by constraining you: instead of struggling to think about which of the thousands of functiosn that might help, you can just pick from a handful that are design to be very likely to be helpful.

In this chapter you'll learn four of the most important dplyr verbs:

* `filter()`
* `mutate()`
* `group_by()`
* `summarise()`

These verbs are easy to learn because they all work the same way: they take a data frame as the first argument, and return a modified data frame. The second and subsequent arguments control the details of the transformation, and are always interpreted in the context of the data frame so you can refer to variables directly. I'll also explain each in the same way: I'll show you a motivating examples using the diamonds, and then give you more details about how the function works, and finish up with some exercises for you to practice your skills with.

You'll also learn how to create data transformation pipelines using `%>%`. `%>%` plays a similar role to `+` in ggplot2: it allows you to solve complex problems by combining small pieces that are easily understood in isolation.
  
This only scratches the surface of dplyr's capabilities but should be enough to get help you do visualisation. The chapter concludes with some pointers to more resources to help you master dplyr for other data manipulation needs.

## Filter observations

It's common to only want to explore one part of a dataset. A great data analysis strategy is to start with just one obseravation unit (one person, one city, etc), and understand how it works before attempting to generalise the conclusion to others. If you ever feel overwhelmed by an analysis this is great technique: zoom down to something that's not overwhelming, master it, and then zoom back out.

Filtering is also useful for extracting outliers. Generally, you don't want to just throw outliers away (often they are were the action is), but it's useful to think about partioning the data into the common and the unusual. You summarise the common to look at the broad trends; you examine the outliers individually to see if you can figure out what's going on.

For example, look at this pair of plots that show how the x and y dimensions of the diamonds are related:

```{r, dev = "png", fig.show = "hold", fig.width = 3, fig.height = 3}
qplot(x, y, data = diamonds) + theme(aspect.ratio = 1)
```

There are around 50,000 points in this dataset: most of them lie along the diagonal, but there are a handful of outliers. One clear set of incorrect values are those diamonds with zero dimensions. We can use `filter()` to pull them out.

```{r}
filter(diamonds, x == 0 | y == 0)
```

This is equivalent to the base R code `diamonds[diamonds$x == 0 | diamonds$y == 0, ]`, but is less repitious because you can `filter()` allows you to refer to bare variable names. 

(If you've used `subset()` before, you'll notice that it has very similar behaviour. The biggest difference is that `subset()` can select both observations and variables, where in dplyr, `filter()` works with exclusuively with observations and `select()` with variables. There are some other subtle differences, but the main advantage to using `filter()` is that it behaves identically to the other dplyr verbs)

Once we've looked at these outliers, we probably want on the clearly good values so we can more clearly see the pattern. To save a little typing, we can take advantage of the fact that multiple arguments to `filter()` are combined with and.

```{r, dev = "png"}
diamonds_ok <- filter(diamonds, x > 0, y > 0, y < 20)
qplot(x, y, data = diamonds_ok) + 
  theme(aspect.ratio = 1) + 
  geom_abline(slope = 1, colour = "red")
```

This plot is now more informative - we can see a very strong relationship between x and y. There are two related challenges with this plot: most of the plot region is empty, because most of the data lies along the diagonal. Additionally, there are some clear bivariate outliers. These outliers are hard to extract because we need some non-obvious function of x and y. We'll solve both of these problem in the next section by adding a new variable that's a transformation of x and y. But before we can do that we need to talk a little more about the details of `filter()`.

### Useful tools

Filter requires a logical vector. You can create a logical vector from characater and numeric variables with the comparison operators:

* `x == y`: x and y are equal.
* `x != y`: x and y are not equal.
* `x %in% c("a", "b", "c")`: x is one of the values in the RHS.
* `x > y`, `x >= y`, `x < y`, `x <= y`: greater than, greater than or equal to,
  less than, less than or equal to.

Comparison operators return a logical vector that contains either TRUE, FALSE, or NA. `filter()` keeps only those observations where the vector contains TRUE (i.e. both FALSE and NA values are dropped). You can convert:

* TRUE to FALSE and FALSE to TRUE with `!x`
* NA to FALSE and TRUE/FALSE to TRUE with `is.na(x)` (more on this shortly).

The logical operators allow you to combine multiple values:

* `x & y`: TRUE if both x and y are TRUE.
* `x | y`: TRUE if either x or y are TRUE.
* `xor(x, y)`: TRUE if either x or y are TRUE, but not both (e__x__clusive or).

Some examples:

* Price less than $500: `price < 500`
* Size between 1 and 2 carats: `carat >=1 & carat < 2`
* Cut is ideal or premium: `cut == "Premium" | cut == "Ideal"`, or 
  `cut %in% c("Premium", "Ideal")` (note that R is case sensitive)
* Worst colour, cut and clarify: `cut == "Fair" & color == "J" & clarity == "SI2"`
  
As well as filter based on the raw values, you can also filter based on functions:

* Size is between 1 and 2 carats: `floor(carat) == 1`
* An average dimension of greater than 3: `(x + y + z) > 3` 

This is useful for simple expression, but as you get more complicated it's better to create a new variable first so you can check that you've done the computation correctly before doing the subsetting.

The rules for NA are a bit trickier, so I'll explain them next.

### Missing values

NA, R's missing value indicator, can be frustrating to work with. R's underlying philosophy is to force you to recognise that you have missing values, and make a deliberate choice to deal with them. In other words missing values never go missing. This is a pain because you almost always want to just get rid of them, but it's a good principle to force you to think about the correct option.  

The most important thing to understand about missing values is that they tend to infect all operations. A missing value means that you don't know what the value is, and if you combine something you don't know with something you do know, you don't know what the answer is:

```{r}
x <- c(1, NA, 2)
x == 1
x > 2
x + 10
```

When you first learn R, you might be tempted to find missing values using `==`:

```{r}
x == NA
x != NA
```

But that doesn't work! A little thought reveals why: there's no just one measurement who's value we don't know. There are an infinite set. Just because two values are missing does not imply they have the same value. Instead you'll need to use `is.na(X)` to determine if a value is missing:

```{r}
is.na(x)
```

`filter()` only includes observations where the logical is TRUE, so NA values are automatically dropped (this turns out to be the most useful default for filtering). If you want to include missing values, be explicit: `x > 10 | is.na(x)`. In other parts of R, you'll sometimes need to convert missing values into FALSE. You can do that with `x > 10 & !is.na(x)`

### Exercises

1.  Practice your filtering skills by:

    * Finding all the diamonds with equal x and y dimensions.
    * A depth between 55 and 70.
    * A carat smaller than the median carat.
    * Cost more than $10,000 per carat
    * Are of good or better quality

1.  Repeat the analysis of outlying values to include the z dimension. Compared
    to x and y, how would you characterise the relationship of x and z, or y and 
    z?

1.  Look at the movies that have a missing budget. How are they different
    from the movies with a budget? (Hint: try a frequency polygon plus 
    `colour = is.na(budget)`.)

1.  What is `NA & FALSE` and `NA | TRUE`? Why? Why doesn't `NA * 0` equal zero? 
    What number times zero does not equal 0?

### Create new variables

To better explore the relationship between x and y, it's useful to "rotate" the plot around so that the data no longer lies along the diagonal. We can do that by creating two new variables: one the represent the relative different between x and y (or in this context the symmetry of the diamond), and its size: the average of x and y.

```{r}
diamonds_ok <- mutate(diamonds_ok,
  sym = log(x / y),
  size = (x + y) / 2
)
diamonds_ok
```

To create new variables we can use `mutate()`. Like `filter()` it takes a data frame as its first argument and returns a data frame. It's second and subsequent arguments are named expressions to generate new variables. Like `filter()` you can refer to variables just by their name, you don't need to also include the name of the dataset.

```{r, dev = "png"}
qplot(size, sym, data = diamonds_ok)
```

This plot has two advantages: we can more easily see the pattern that the bulk on the points lie on, and we can easily select outliers. Here, it doesn't seem important whether the outliers are positive (i.e. x is bigger than y) or negative (i.e. y is bigger x). So we can use the absolute value of the symmetry variable to pull out the outliers. 0.05 seems a reasonable threshold. We'll check out the results with a histogram.

```{r}
qplot(size, abs(sym), data = diamonds_ok)
diamonds_ok2 <- filter(diamonds_ok, abs(sym) < 0.05)
qplot(sym, data = diamonds_ok2, binwidth = 0.001) 
```

That's an interesting histogram! While most diamonds are close to being symmetric there are very very few that are perfectly symmetric (i.e. `x == y`.)

### Useful tools

Often useful to think about decomposing a variable (or pair of variables) into an alternative form. Many transformations are domain specific, but there are many that are useful in a suprisingly wide range of circumstances.

* `sign(x)` + `abs(x)`

* Log-transformations in general are extremely useful - they convert
  additive relationships to multiplicative. The squish extremely long-tails.
  Convert power relationships to linear. 
  Good examples at <http://stats.stackexchange.com/questions/27951>

* If you're interested in the relative difference between two variables, the 
  best summary to use is `log(x / y)`. It's the only symmetric, additive 
  and normed measurement (@tornqvist:1985).
  
* Another useful transformation is to partition into trend and residuals
  with a linear model or otherwise. You'll learn more about that in the
  next chapter on modelling.
  
* Sometimes changing to polar coordiantes, distance (`sqrt(x^2 + y^2)`) and 
  angle (`atan2(y, x)`) can be useful.
  
* Sometimes integrals or derivative might be more useful - if you have
  distance and time, would speed or acceleration be more useful? (or vice versa)

It's always worthwhile to think about potential transformations that may make more clear interesting signals in your data. If you focus on interpretible transformations, i.e. things that make it easier to understand, and not on randomly apply every transformationt that you can think of you are less likely to run into statisitcal problems ("torturing the data until it confesses" -- Tukey.)

### Exercises

1. Practice your variable creation skills by creating new variables

    * The approximate volume of the diamond (using x, y, and z).
    * The approximate density of the diamond.
    * The price per carat.
    * Log transformation of carat and price.
    
1.  The depth variable is just the width of the diamond (average of x and y)
    divided by its height (z) multiplied by 100 and round to the nearest 
    integer. Compute the depth yourself and compare it to the existing depth
    variable. Summarise your findings with a plot.

### Group-wise summarise

Many insightful visualisations require that you reduce the full dataset down to a meaningful summary. Two functions from dplyr allow you to do that: `group_by()` describes how you want to break your data down into groups, and `summarise()` reduces each group to a single row. For example, to look at the average price per colour:

```{r}
by_cut <- diamonds %>%
  group_by(color, cut) %>%
  summarise(n = n(), price = mean(price))
by_cut
ggplot(by_cut, aes(color, n, group = cut, colour = cut)) + 
  geom_line() + 
  geom_point()
```

### Grouping

(Grouped mutates and filters are also useful, but more advanced. See the window function vignette for more details. )

Useful summary functions:

* counts: `n()`, `n_distinct()`
* central tendency: `mean()`, `median()`.
* spread: `sd()`, `mad()`
* extremes: `quartile()`, `min()`, `max()`

Another extremely useful technique is to use `sum()` or `mean()` with logical vectors. When logical vectors are treated as numeric, TRUE becomes 1 and FALSE becomes 0. This means that `sum()` tells you the number of true elements, and `mean()` tells you the proportion of true elements. 

For example, the following code counts the number of diamonds with carat greater than or equal to 4, and the proportion of diamonds that cost less than $1000.

```{r}
summarise(diamonds, 
  n_big = sum(carat >= 4), 
  prop_cheap = mean(price < 1000)
)
```

Most summary functions have a `na.rm` argument: `na.rm = TRUE` tells the summary function to remove any missing values prior to summiarisation. This is a convenient shortcut: rather than removing the missing values then summarising, you can do it in one step.

### Statistical considerations

When summarising with the mean, it's always a good idea to include a count and some measure of uncertainty. This helps you calibrate your assessments - if you don't include them you're likely to think that the data is less variable than it really is, and potentially draw unwarranted conclusions.

```{r}
by_cut <- diamonds %>%
  group_by(color) %>%
  summarise(
    n = n(), 
    med = as.numeric(median(price)), 
    lq = quantile(price, 0.25), 
    uq = quantile(price, 0.75)
  )
by_cut
ggplot(by_cut, aes(color, med, group = 1)) + 
  geom_linerange(aes(ymin = lq, ymax = uq), colour = "grey50") + 
  geom_point(aes(size = n)) + 
  geom_line() + 
  scale_size_area()
```

### Exercises

1.  For each year in the movies data, determine the percent of movies with 
    missing budgets. Visualise the result.

1.  For each combination of diamond quality (e.g. cut, colour and clarity), 
    count the number of diamonds, the average price and the average size. 
    Visualise the results.


## Transformation pipelines

In only the simplest of cases will you apply just one dplyr verbs. In most real analyses, you'll end up stringing together multiple `mutate()`s, `filter()`s, and `group_by()`s `summarise()`s. There are two basic ways you can create such a sequence.

```{r}
# By "composing" functions


# By using intermediate values

```

Both approaches have problems. Function composition is hard to read...


To make this easier, dplyr provides the __pipe__, `%>%`. It allows you to turn function composition into a sequence of transformations.



### Exercises

1.  Translate the following code to use the pipe. Describe what it does in 
    English.

1.  What does the following pipe do?

    ```{r}
    library(magrittr)
    x <- runif(100)
    x %>%
      subtract(mean(.)) %>%
      raise_to_power(2) %>%
      mean() %>%
      sqrt()
    ```

## Other verbs

There are two other verbs that are less useful for visualisation:

* `arrange()`, that can be useful when you're looking at the data from the 
  console. It doesn't affect visualisations because ggplot2 doesn't care about 
  the order of the rows. 
  
* `select()` picks variables based on their names. Useful when you have 
  very many variables and want to focus on just a few for analysis.

There are two that are variations on `mutate()` and `select()`:

* `rename()`

* `transmute()`

## Learning more

This has given you the basics for doing single table data manip in R. Learn more:

* Cheatsheet

* About using dplyr with databases.

* The verbs that work with two tables at a time: mutating joins, filtering
  joins and the set operations.
  
* Group-wise filters and mutate can also be useful, particularly in
  conjunction with window functions.

* The dplyr vignettes, which you can see with 
  `browseVignettes(package = "dplyr")`, go into considerably more depth into
  dplyr. There you'll learn how to also use dplyr with data frames, and how
  to work with multiple tables of data.

